# LECTURE 2
    Lexical Semantics: Meaning of word
        lemma(title), sense(different meaning within diff context), definition
        connections between words (synonyms, opposites, etc)
        taxonomy - absract(general words) to concrete(specific).
        Semantic frames and roles -> to denote perspective or participants in an event.
        
    Problems with discrete repr:
        one hot vector(give each word separete numbers on vectors for 1): sparce, subj, expensive, hard to guess relationships.
        goal: to map similar meaning workds closely, can use KNN to find close words. Standard way to repr mean ing in NLP.

    Training of word embeddings:
        Wrod2vec: 
                Cbow(opp of skip gram) - from many guess one.
                Skip Gram: use imput to geuss bunch of words (define a sliding window and train a classifier with padding, 
                            to predict correct token for all sectences. roughly self supervised.)
                            - some words used a lot, learn a lot of repr in language.
                            - others used less, more concrete, meaning very unique.
                            uses ONE HOT + embeddings to train/predict surrounding/similar word repr.
                            - loss function log probablity(dot product to predict similarity between surrounding words and 
                                    normalize with all words -> expensive so approx by negative samplinig(sampling without main 
                                    sliding window but vocab is not all unqiue - but can use freq for norm) idea grab random samples 
                                    to do sum of pairwise normalizaiton -> convert to binary classification-easier to optimize and 
                                    learn repr) then use some gradient decent to predict.
                            - evaluation: check quality: instrinsic eval- test repr align with intuition about word meaning, using cosine similarity of word embeddings.
                                        extrinsic eval: test weather repr good for downstream tasks, such as taggin, parsing, QA or some task used by some model.
    Text Classificaiton: generative vs discriminative.
            generative - prob joined with and ouput, argmax over joint prob.
                - prob of input first, probably more expensive.
            discriminative - prob output given the inout data, argmax over conditional prob.
                = directly models prob with output space.
                - BOW(bag of words) discriminative model, does not care about word order. get score to prob then gradient descent. but for neural networks can use sequential order.
                
            logistic regression good for binary classification. sigmoid for binary, softmax for muti-class to repr to valid probabilty.
            gradient descent - gradient of loss function and use chain rule. to move to direction that decreases the loss. lr is the speed of movement.

    Q: norm samlnig denom is some sort of avg to shorten error?
    Q: so do we use freq when norming or no? since Vocab is not unique?
    Q: how does input become smaller dim(PCA?)?

    HW1 classification example:
        Continous bag of words(CBOW) has lower dimension, just bag of words(BOW) is one hot size of each lookup is number 
        of vocab. + linear layers. then softmax function and optimize with gradient descent and regulizer.
# LECTURE 3 : Language Modeling
    Ngram Language model, Neural Model, Evaluation

    define: final end of sentence token is special. 
        problem is probability of grabbing sentence from 

    What can it do: score sentences(high/low), or generate sentences(prob: of next work/token)

    N-gram Language model:
        every word is independence, and can use maximum likelihood MLE to see how likely eaach word is supposed to appear(by counting).
        - allows prediction of whole sentence too.
        - want to score with unknown read words,  interpolations w/ UNK model enables to allow low frequency unknown words.

    higher order model: limit context length to smaller window(n-1). MLE is count and divide. linear interpolation to deal with zero counts.
        sometimes add one smoothing, every word adds 1 count.

        other smoothing techniques: additive, discounting, kneser-ney. most of them to address zero count issue.
    
    Limitations of Ngram:
        Ngram cannot share strength among similar words, need class based language models.
        cannot condition on context with intervening words. need skipgram language model.
        cannot handle long-distance dependencies, need cache,trigger, topic, syntatic moels, etc.

    Linear models:
        cant learn feature combinations, logical errors - cant be expressed by linear features, use neural language models.

    Neural ngram model(since proximating current based on previous, but using neural model to learn embeddings):
        convert word predition to discriminative text classification.

        feed-forwrad neural language models. - uses previous words, looks up word embeddings, combine linear vector to get intermediate 
        vector + bias to get score and then get probability to get next token.
            (grab two d dim vectors and grad a d vector absed on that, convert to v(vocab) dim, softmax to prob)
        
    
        Solves: simililar word/context issues, intervening words, cannot handle long-distance dependencies.

    neural networks allow design of complex functions.

    PROS ngram: models are extremely fast, better at modeling low frequency - TOOL: kenlm

    ngram lm(non parametric-non training just statistics), prefix context table and counts will be massive to store. to compute just retrieve row.

    neural counts but also compresses, memorizes and generizes.

    Non-parametric: • Memorize low-frequency pairs of (prefix, next word) • Cannot generalize to unseen pairs • Interpretable & easy for incremental updates (insert/delete/replace)
    Parametric: • Learning from data-driventraining • Good generalization tounseen pairs • Black-box & expensive forparameter updates withforgetting issues

    Why llms so powerful: 
        - Architecutre, MLP then RNN and now transformers to capture sectquecnes and effecienceis to scale to 100B+.
        - Pre-training on massive raw texts.
        - Fine-tuning on language intrustiosn with supervised learning or RLWF.

    LM Evaluation:
        - log prob of sentence over test corpus, aggregate them log likelihood score(not comparable to different test sets), so we normalize it by count, turn into per word cross entropy and grap perplexity.

        unknown words: 90% of words are low freq, cut size of corpus(by unk or rank threshold), treat 90% as unknown words, to save compute.
            - subword tokens can handle loss of accuracy by using sub tokens instead of throwing tokens away, also handles mispellings.
        
        IMP: cannot compare models over different vocab. 
            Or more accurately, all models must be able to generate the test set (it’s OK if they can generate more than the test set, but not less)
            • e.g. Comparing a character-based model to a word-based model is fair, but not vice-versa

# LECTURE 4: Recurrent Neural Network 

    General knowledge:
    - when working with minibatches. need to add artificial token to make sure each sentence has same length. add paddings. (PER BATCH ONLY)
        - when computing avg we should zero out padding tokens, sum out embeddings over word and divide by number of real tokens-non paded length.

    Sequential:
        long distance dependencies in language can be compicated, nouns, or references such as 'it'.
    
    Recurrent Neural Networks:
        Approaches:
        1. variable size inputs: add zero paddings and concatate + Linear layer -> simple but downt works for longer sequences. massive ff layer.
        
        2. feed one word to each feed forwawrd layer : for each token dot product with weight + bais, and pass to next token. 
                (idea compress and pass info to next token to carry on, avoids super long W matrix.)
                do not need to pad senteces, and each ff layer is smaller than previous method.
                maiin idea for RNN, we can have many layers as we want, variable depth network. remembers information along time dimention.
                we can predict per token vs label, grab loss and sum them up for total used in backprop. backprop updated weights(rnn) not inital hidden layer passed on.
                backprop over time sequence - issue vanishing/exloding gradients. if we do back prop since al weights are shared.
                    - handling vanishing gradient: LSTM - long short term memory. make additive connections between time steps. gate to contril info flow, and 2 mem cell to store short/long term info.
                        - forget gate-how much forget long time mem, input gate-how much to add to long term mem, compute new values to add to memory, output gate-how much to pass on from long term to short(next step).
                        - all is passed to next time step(short term - only some portion of long term-higher freq of updating, allows prediction per token in seq).
                    - handling exploding gradient: gradient clipping(general idea can be applied to others). we norm by some number before updating, basically making it take shorter steps, instead of massive ones in gradient descent.

                exposure bias/distributional shift: training we pass in ground truth per token, but during test we pass on predicted words to predent/finish sentence. since its a prob dist model might sample an unknown sequence.
                    - happens for generative tasks, not classification.
                    - to handle:some reinforcement learning or scheduled sampling. first train on ground truth for some iter but gradually switch to taking its own input, and always continue like that. 
                                        - cannot start with pass in its own input, or else it will learn garbage. start with supervised then reinforcement function.
                                        - handles model robustness to handle new sequences, not necessarily accuracy. 

                                        Q: do we have to write our own feedback func to say weather new seqs in training during exploration are good or not?
                                            - model is baked in to explore some similar but wrong words but should later still learn the proper sequence. so overall message is not lost. not necessarily RL but SL + noise.
                
                Applications:  represent a whole sentence to make pred, represent a context within a sentence(read context up intil that point).
                                - can do 1-1, 1-many, many-1, many-many, many to many fully conencted.
                                    image captioning, sentence classification, machine translation, sentance tagging.
                                
                                - can do binary or multi class prediction.sentence repr for retrival, sentence comparison, etc.
                                - sequence labeling, calculating reprs for parsing.
                                - language modeling - tagging task, each tag is next word. is a auto regressive model(curr depends on prev token.)

                                bidirectional rnns - to handle two directional languages, also llms use it, also useful for sentence repr.


                What can LSTMS learn: 
                ...

                Efficiency tricks: minibatching makes thigns faster. harder in rnns than feed forward networks. since each word depends on prev word. (better to separe padding and end of sentence token)
                                - using a masking token to zero out paddings. to exclude it when calc loss. and divide sum of loss by real tokens in minibatch.
                            
                            bucketing/sorting - put sentences of same length with same minibatch. but can result in decreased performance.
                
                handling longer sequences: cant fit massive lang in mem, truncated segments. separete it into 2 passes but connect end of 1st to start of 2nd's hidden layer.
                                            - backprop individually though.
                
                Gated Recurrent Units: LSTMS but way more efficient. with better parameters.
                architecture search - basic lstms pretty good(the various combinatoins gates searched.)

                Multi-Scale Pyramid RNNS: one for word, one on top for sentences, one for paragraphs.

# LECTURE 5: Attention and transformers
    Encoder - Decoder models: used to be popular, nowadays only decoder(encoder merged within). prob, compressing sentences of varying length to single vector, hard to decode later on.
        - idea use mulitple vectors bsed on length of sentence: attention.
    Attention: embed vocab to dict, query a target element, pick relevant sources by comparing query/key-value based on confidenc. summarize relevant work tokens into a contetxt vector, used to predict target token.
        encoder gives keys and vals to each word based on learned functions, decoder creates queries to check/find relevant key. we dont use argmax on keys (not differentiable(is discrete)) instead use softmax (to smooth attention dist), is differentiable, allow backprop.
        after grabbing query results, we grab norm attention weights, create a context vector with weights + base word vectors, and pass it though a FF network + predicting base word vectors to predict per label. 
            Feed Forward:
            - multi layer perceptron great for large data (applied position-wise after attention)

            Score (attention alignment):
            - dot product; if too large dim, use scaled dot product
            - bilinear (general form): score(q,k) = qᵀ W k
            - MLP/additive (Bahdanau): score(q,k) = vᵀ tanh(W_q q + W_k k)
    
    transformers: adv: really fast, only matix, parallel sequetial processning, efficeint.
        - self-attention - allwos parallel comp of all tokens.
            attends to itself most, but also givs attention to other words/elements in sentence, context sensitive.
            using embeddings to grab key,query,value. use querry to grab ino from each other embedding, use softmax on all. use values and softmax dist to grab sum. saically summarizes its own info and other words info. can stack multiple self-attention layer.
        - multi headed attentions - allows querying multiple positions at each layer.
            multiple attention heads focus on different parts of sentence(can encode in parallel).used several heads per set, t heads to create a single d(t) dim vector.
            add nonlinearity - FF(MLP), so output is no longer liner to input, add capacity to model to make it complex. we stack self-attention(multi head) and non-linear layer N times to increase capacity.
        - position encoding - addds position ino for each token.
            model perspective no position info, since parallel. add soe info function at the very beginning to augment input where word is location.
            naive = integer index and another dim to input. issues embedding vector also numeric, so may cause instability.
            we use frequency based functions, sigmoids with diff frequency. differnet dimentions say differnet info. 1st even odd indicator, layer first-half etc. each word have added dims. combination if some of those dims give positional info. typically concatate with word embedding info and pass them as same.
            another idea add learnable parameter for every input. but fixed size for length, need to estimate or model might not work. typically more optimal but more complex.
            typicall absolute not useful, relative to other words more useful.
        - adding non-linearrities - combiens features from self-attention layers.(expl above)
        - masked decoding - prevents attention lookups in the future tokens, to avoid predicting future tokens.
            avoids looking into future tokens, masks attention to future tokens(keys).(step before making softmax dist). future token will not affect context.

        self attnetion: combine words
        multi-head = learn independ head information
        norm dot prod atten - to remove baias in dot prod calc
        prod encoding - without RNN, to udnerstand location.

        Training tricks: layer norm - avoid number blowouts, training schedule - adjust from adam (study show needs some warmup from small to large then exp decay to find optimal lr), llabel smoothing - insert some uncertainty to training proc(allows sampling since finding a single point is rly hard).

        Attention hard - slow at test, doesnt necessarily outputform RNNs on decoder side for seq2seq taks (transformer need a lot more data), hard to train on small data.

        EXTRAS:
        Additions: can add markov properties i.e. add attention matrix. encourage sparcity: instead of softmax to smooth out, ask model to pick a few tokens instead each time.
            incorporating markov properties: attention from last time correlated with attention from this time. add attention from last for next deision.

            Hard attention: instead of softmax, use zero-one decision. harder to train, needs Reinforcement learning, perhaps helps interpretability.

        Better Training: 
            neural models tend to drop or repeat content, add penanty if attention not 1 over each word.

            attention is not alighment, often blurred, off by one on matrix for translation, and other tasks too.

            supervised - pretrained + supervised is strong alignment model. but tons of data, model can generally learn too.

        What else:
            Copy Mechanism: model can not only generate from vocab, but also copy tokens directly from input/history, final prob = mix of generate prob + copy prob

            hierarchical attentions: attention over words then attention over sentences then document. grab context of document.

            attention allows various modalities, vision, text, sound/freq, etc.

            multi source attention.

            basic idea of attention: idea similar to info retrival.

# LECTURE 6: Pre-traind Sentence and Contextualized Word Reprs.

    why pre-training, 

    non-Contextualized - word2vev, seq ,etc.
    contextualized - bert etc.

    pass non-context though an multi layer perceptron to turn into contexualized?

    why pre-trainng: learn multi-tatsk learning, transfer learning - finetuning

    NLP TASKS: text - language modeling, Natural occuring data - machine translation, hand-labeled data - most analysis tasks.
        - 1. Perform multi-tasking when one of your two tasks has greatly fewer data, 2. Perform multi-tasking when your tasks are related

    multi-task leaning, train reprs to do well of multiple tasks at once, i.e. tagging, language modeling.
        - should be table to train encoder on large dataset for one obj, then use same encoder+finetuning later for other tasks such as tagging.

    3 main objtives - define model, define training obj, data.

    end-to-end - is done if easy or has enough data points. else always use pre-training.

    Usage of sentence reprs: 
        Sentence Classifcation - sentiiment, classify, binary labels over sentence, etc.
        Paraphrase identification/Semantic Similarity - compare meaning semantically between phrases
        Textual Entailment - learn implications(Entailment), Contraditions, Neutral.

    Pretraining Techniques:
        (idea model pretrained for cetain task, then encoder/embeddings used from that to be used in another task)
        eg 1: Google: model: LSTM, obj: LM obj, data: Classification data itself or amazon reviews. downsteam on text classficaion, init weights and continue training.
        eg2: GPT: Masked self-attention(transformer), LM obj, Data: BooksCorpus. downstream some task finetuning other tasks additioanl multi-sentence training.
        eg3: Auto encoder + Transfer: LSTM, from single sentenve vec re-construct the sentence. Data: clasification data itself or amazon reviews. downstream on text classification init weights continue training.
        eg4: Skip-thought vectors(sentence level contet): lstm, predict surrounding sentences, data books, imp because of context. downstream sentence pair classification. 
                (Similar to Skip-gram that predict the surrounding words by the center words)
        eg5: paraphrase based contrastive learning: try several model, predict weather two phrase are paraphrases or not, data: paraphrase db from bilingual data.downstream: sentence similariy, classification, 
                (LSTMs work well on indomain but word averaging generalizes better.)

        deep-fusion - concat sentence for grabing self-attention(sentiment - close, deep,repr, efficient)
        late-fution - do encoding on ecah then gram.(sentence classification - sparse, more time?)

        eg6: Entailment + Transer:Bi-LSTM + max pooling, (use human labels for pretraining) supervised training for a task such as entailment learn generalizable embeddings. Data: Stanford NLI, MultiNLI, 
                (Tends to be better than unsupervised objectives such as SkipThought)

    Context word repr:
        Central Word Prediction:
            context2vec: Bi-directional LSTM , Predict the word given context, data: 2B word ukWaC corpus. downsteam  use vectors for sentence completion, word sense disambiguation, etc
            ELMo: Model: Multi-layer bi-directional LSTM • Objective: Predict the next word left- >right, next word right->left independently 
            Data: 1B word benchmark LM dataset. Downstream: Finetune the weights of the linear combination of layers on the downstream task

    Masked Word Prediction:
        Masked Word Prediction (BERT):model: Multi-layer self-attention. Input sentence or pair, w/ [CLS] token, subword representation, obj: Masked word prediction(mask certain words ask predict) + nextsentence prediction((are they close or far))
            Data: BooksCorpus + English Wikipedia (16GB)
        
            select 15% of words randoly, replace with mask 80%. time, 10% replace random(to simpulate noise), 10 no change. predict all maskd words given surrounding context. like context2vec but better suited for multi-layer self attention.

        Consecutivee Sentence Prediction: classify two sentence as concsec or not. data = 50% of trainngin from openbooks is consecutive, use unique cls token to include meaingn of two sentence. mask some sentece in sentence and sep tokens to separaee sentences.  and binary predictions.

        Hyperparameter Optimization/Data(RoBERTa) - same as BERT, but trainlonger and drop sentence prediction objective. but with much larger data. empirically much better.
            - sentence prediction is easy(doesnt help) if mask token prediction is done well(hard-loss goes down slow).

        Distribution Discrimination -ELECTRA: same as bert,Sample words from language model, try to discriminate which words are sampled. discriminator used as word encoding model later on.
            (masked sentence passed in generator, which predicts/sample words, discriminator then predicts if original word passed or not.) 
            (better than bert, more gradient updates to model since per token labeled, bert only does 50% as their tasks masks 50% only)

        Permutation-based Auto-regressive Model+ Long Context(XL-Net):Same as BERT, but include longer context, Predict words in order(itself and before), but different order every time(train sentences with different permutation orders: some noise but able to learn differnt perspectives). Data: 39B tokens from Books, Wikipedia and Web 
            - later version, instead of shuffing randomly, it group phrases to avoid random places, avoid learning wrong word combinations in sentence.
        
    Compact Pre-trained Models: 
        ALBERT (Lan et al. 2019): Smaller embeddings,and parameter sharing across all layers, but the same inference time as the BERT counterpart 
        DistilBERT (Sanh et al. 2019): Train a model to match the distribution of regular BERT
   
    SUMMARIES:
        Which model:
            - Averaging word embeddings can outperform more complex models, especially on out-of-domain data.
            - BERT shows the strength of bidirectional transformers but does not benchmark against LSTM models like ELMo due to lSTMs being constrained by computational efficiency and scalability.
            - With BERT-like training data, their ablations show architecture and training tweaks can yield further improvements.

        Which Training Objective? chatgpt
            - When using the same data, bidirectional language modeling outperforms a machine translation encoder.
            - Next-sentence prediction, combined with masked LM, seemed helpful in the original BERT paper(boosting perf), but later work (e.g., RoBERTa) found it unnecessary and even limiting when scaling training.
            - Next-sentence prediction provides little to no benefit and can be dropped.

        Which Data?
            - Preliminary results suggest more data generally improves performance.
            - Scaling with large web data yields gains but inconsistently.
                Data with context is probably essential/best.

        Pre-trained Large Language Models: 
            GPT-3: similar to GPT-2, but pre-trained on lot more data using autoregressive LM objective. Demonstrate good few-shot in-context prediction ability.
            ChatGPT: init form GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF).

# LECTURE 7:Parameter-efficient Fine-tuning - and other ways to bypass costly fine-training.
    PEFT - Fine tune a small amount of mdoel params, instead of entire model. on a small dataset of downstream tasks. Other parameters are frozen.
        pors - reduce compute and sotrage costs, 
        mitigate catasprohic forgetting
        easy to update model to new data and facts
        better perfm on low data
        comparable perom to finetuning
    
                                PEFT                    | Full Fine-tuning
    Learnable parameters:         A small subset        | Entire model
    Training Performance:         Close to fine-tuning  | …is fine-tuning :P
    Training Data:         Small                        | Large
    Training Time:         Faster                       | Longer training time
    Overfitting / forgetting: Less prone to overfitting | More prone to overfitting

    3 ways to adapt model capacity(Three Computation Functions): 
        function composition - add extra functions (combime multile as well) on top on NLP layer, (new function layer)
            - i.e. Adapters - additional modeluse in layer - match or outperfms fine tunign.
        input comp - combine data as input to create longer sequence. has effect on self-attention layer. (adapt data)
            - i.e. prompt tuning, prefix tuning (beginning of sentence instead of end)- context window of model is increases - good for large models
        parameter comp - low rank extra params in middle of the model, in matrix math area to increase wight by adapter wight. (adapt model with addl func)
            - i.e. Lora/QLora - no increase in model size, good.

    Function Composition:
        Adapter - adapter layer after each FF layer (before layer norm if there). Adapter itself is an multiplayer -2 layer perception.
            - each layer is a matrix which transforms the input in new space. adjust embedding direction. helsp reroute the data embeddings to what the "upper layer expects."
            - increase capacity of model to learn new date.
            - vs. full finetunign. small data better prfm with adapter, and about equal perfm with large data. better accury with fewer num of params updated. embeddings repr shifts little(origial meaning holds?) vs finetuning.
        
        IA^3: Add learnable params to self-attention layer instead on NLP, - model selects params that are moe and less imp for given downstream task.
            - adds learnable layer to MLP layer too.

    Input Composition:
        Prompting with text: prepending instructive words before actula test input. 
            - text tokens as learnabale params.
            - trigger mdoeling to learn tasks- 
            - models are sensitive to the formulation of the prompt and to the order of examples
            - Prompt Tuning: 
            Prompt tuning: only updates a small task-specific prompt parameters(learned, i.e. extra params added to left of embedding) for each task, enables mixed task inference.
                - during testing, add learned prompt to input.
                - as model size increases the perf of prompt tunign matching full fine-tuning.
                    - naivly few shotting model is still wrose than both.
                - only updates prompts, not x inputs concated.
            Prefix Tuning - more effective to capture gradient updates over layers.
                - adds larnable params at the beggingin on the input sequence of all layers. (multi layer )
                - better in low data regiuemes, better in general too - full finetunign.
                - only prefix updated, rest model frozen.
    
        Q: inputs trained to or just added to front.
            - no inputs trained, empty/learnable parameters added to front after capturing sentence embedding
        Q; prompts multiple voab or just 1
            - 1 or multiple your choice? More training?
        Q: Cost of adding adapters and input prompting compute costs?
            - there is added costs/params to both methods, but negligible.
        Q: Combination of these better in some instances or typically by itself better.
            - not necessarily, main idea is to find the sensitive spots and adjust them. There is a paper that is trying to find the spots and test multiple methods/combination to see if any better. Auto smth?
        Q: does fine tuning increase inferece speeds compared to original foundation model?
            - not inference speed, just training speed.
        Q: Does lora have any impact on forgetting? since we are adding to params? 
            - if matrix conflicts with other for the updates, depends on tasks if the knowledge confict w/ pre-tain knowledge. All methods can have forgetting no gurantee/non-gurantee.
        Q: Qlora explanation - quantization has some lsoss, previous does it affect performance? what dooes the bits reprepsent.
            - bits represent weights, we shink the specificity of the weights to a smaller "scientific number", not massive loss in performance if quantization done properly.
    Parameter Composition:
        Low Rank Adaptation (LORA)- approx self attenupdat of lernable weight by a low -rank matrix. initi update is 0, after training updates are added 
                back to original checkpoint. so at inference cost of the updated checkpoint is samea s orig checkpoint.
            -   save a lot of params, no actual overhead in inference. small overhead to do computation during inference.

        LoRa works better than other PEFTs (adapters, full finetunign, - prelayer, etc).
        why works: full tuning update typically happen in some low rank space, so we can try to update with low ranks as well.
        pretraining provides strong initization in D dimm space, model only needs to explore subspace during fine-tuning to learn final weights.
        Reduces VRAm consuption, fewer gpus and fewer I/O operations.
        lora still reqs forward comp and back prop. so lora speeds up 25% but noto 10x compared to fine-tuning
        
        QLORA - Convert information in a high-precision data type to a low-precision data type, Allows training a LLM in a single consumer GPU 33B in a single 24GB gpu.
            - lower precision avg. nowdadays. use fload 8 or mixed precision. - typically dont need very accurate steam for numbers.
            takes 32 bits to some rounded 8 bit representations, then after gradient updating on smaller bit dequant it back to fp32 representations.
            Double Quant: quantize the fp32 constant inside the fp32 bit, to further save memory on top of quantizing weight matrix blocks. dequant will have an extra nested step as well.
            quantization hsa some information loss, so we do blockwise quantization after flattening matrix(to avoid the max working on a large matrix), to minimize loss.
                - more sensitive to Hyperparameter, perm not same as full-finetunign due to agressive compression.
            can match certain fine-tuning methods with good methods.

        Check picture grpah to get and idea of eah model acc vs num of param.
            - depends on usecase which method better than others.

        Other ways to bypass expensive fine-tuning:
            - PEFT is one way to adapt model to downsteam taks
            - prompting incontext learning - doesnt require furthier training.
            - Retrieval Augment Generation(RAG) to steer model perfm - need to update model to use actual data.
            - Model editing - tuning free approach to modify behavior
                - computationally innexpensive, targetted modification to params. - if we understand causal mechanisms on internal layer
                - cons - kinda like blackbox. hard to understand?
                - Editing knowledge editing - knowledge store or update weights(locate and edit, constrained tuning)
                    behavior - inference time , activation editing, decoding time.
            
        Training models are sene as vectors: has additive propertives, skills embeeded in linear subspace, can combine to explore anothter trajectoty.
            - can take negation of task to unlearn, 
                - Can reinforce certan skills, or reduce certain skills. with combinatiosns (+ then (-), etc.)
        
        can steer model on self atten layer, modify attention heads. need good understandign of heads to steer or remove heads.
            - (incase its persay learning harmful info/hateful speech) can do targetted ediitng as well.
            - targetting heading better than fine tuning and other approaches as well.

# LECTURE 8: Instruction Tuning and Multitask Learning
    several tasks in nlp, only text-for language modeling.Naturally occurring data: e.g. machine translation. Hand-labeled data: e.g. most analysis tasks, 
    multitask: we can train models on many data to do well on several tasks.
    pretrain finetune: we can pretrain on one task and then finetune for later.
    prompting: Train on LM task, make predictions in textualized tasks.
    instruction tuning: Pre-train, then fine-tune on many different tasks, with an instruction specifying the task.
    
    Basic Fine Tuning: Build a model that is good at performing a single task
    Instruction Tuning: Build a generalist model that is good at many tasks

    Examples to build/evaluate machine understanding instead of just memorization.
        give context free question answering dataset/open-book QA, answer a prediction wihtout any specific groundnig knowlegde.
        give contextual question answering/machine reading/closed booking qa: Answer a question about a document or document collection
        Code generations: generate code from natural language command.
        Summarization: single doc-compress longer doc to shorter, multidoc-compress multiple to one.
        Info extraction: entity recog - identify which words are entities. Entity Linking - connect entity mentioned to another knowledge bases. 
            Entity coRef - find which entity in input correcpond to each other(him mean obama, etc.). Event Regoc/linking/coRed - identify what events occurred.
        Translation: from one lanuage to another. quality assessment done using similarity to ref translation.
        General Purpose Benchmarks - collection of general NLP tasks of past decade used to benchmark. to guage ability across broad range of tasks.

    MultiTask Learning: Perform multi-tasking when one of your two tasks has fewer data : High-resourced language → low-resourced language, General domain → specific domain.

    Domain - one task but data might be from several areas. sometimes labeled other times now.
        in practice: Content, what is being discussed.Style, the way in which it is being discussed. Labeling Standards, the way that the same data is labeled

        Types of shift: Covariate Shift: The input changes but not the labeling. Label Shift: The output dist diff (which also implies the input changes). Concept Shift: The conditional distribution of labels changes (e.g. different labeling standards).

        Generalization: 
            Domain adaptiation-train on many adapt to a target domain at testing. 
                Supervised adaptation: train w/ target-domain labeled data. Unsupervised adaptation: train w/o target-domain labeled data
            Domain robustness-train on many doms perform well on all doms.
                esp minority doms.Zero-shot robustness to domains not in training data.
        Detection: Binary classification: detect whether a test example is an OOD example or not.
        
        Multilingual learning also a multi task learning.

        i.e.: Languages as Domains: Multilingual learning is an extreme variety, different language = different domain
            Adaptation: Improve accuracy on lower-resource languages by transferring knowledge from higher-resource languages 
            Robustness: Use one model for all languages, instead of one for each

        Earlier Method on Multitask Learning Feature Space Regularization: Try to regularize the features spaces learned to be closer to each-other.
            encourage rep learning. classifier to learn class label on top of adversarial feature extractor for downstream tasks. from end of featrue extractior we also put domain classifier to encourage((to check)) domain specific learning(and to avoiding mushing everything/all domain in same space).
            idea: want the encoder to learn representations useful for the task. But also want the encoder not to “overfit” to one domain if you want transfer/invariance
                label classsifier minimizes loss from both feature extractor and label classifier to make feature predictions on downstream labels.
                domain classifier minimizes loss on bomain classifier but maximiszes loss on feature side, to encourage features that are hard for domain classifer to distinguish - domain invariant.
                    i think the idea is if the feature extractor learns representations that confuse the domain classifier, then those features must capture what’s common across domains, making them more general and transferable.
                Encoder basically minimizing label loss while maximizing domain loss so those features hide domain cues and become domain-invariant.
            if domain specifc invariance is good, can be confident label classifier will also be able to generalize better.
            neding underlying similarity to pass down task - cant work in unbalanced, un related, etc. can have neg transfer-degrade perf. need underlying assumption of some similarity/structure to enable transfer. 

        Simpler Multi-task Learning Method(data manipulation): i.e.Adding Domain Tags: add tags to indicate what domain or language let model know aht it would want it to do later. Model still doing task prediciton. introduces a small number of params.

    Instruction Tuning:
        T5: Text-to-Text Transfer Transformer -Supervised training on many NLP tasks, Control a single model to do tasks following the instructions in the prompt.
        T0 model (Sanh et al 2021) from Hugging Face& google: Multitask Prompted Training Enables Zero-shot Task Generalization. i.e. trained on summarization, sentiment, question answering but was able to do inference on natural language inferece
            can improve zero-shot perfomrnace than gpt zeroshot and gpt fewshot.
            FLAN Collection: instruction tuning dataset.
            models ex: Flan T5, LLaMa2 Chat, Mixtral Instruct

    Synthetic Data generations: 
        Self-Instruct: to generate syn data and train model again.
            Models sees all tasks, generates synthetic tasks instructions, then ask model to follow isntruction some classifications, and create outputs(ouptut first/input first), then filtered ans passed back in pool again.
                i.e.Input-first: Ask an LLM to come up with the input fields first based on the instruction, then produce the output. (or can use some label and then ask to generate text based on that label i.e. sentiment, etc)
                    Output-first: Generate the output label, and then generate the input based on the output.
    
        Evol-Instruct:  idea of evolve instruction into more complex instructions.
            filter out invalid instruction, find similar instruction but not smae intent, etc.
                indepth - you can rewrite given prompt into more complex version to make tasks more challengin even though same intent.
                in-breadth evolving - draw inspiration and create a completely new prompt same domain but even more rare.
            used to create new instruction pool to increase reasoning examples, deepening, complicate, concretizie, add constraints, etc.

    Knowledge distillination - large model to small model to run deployable env.
