# LECTURE 2
    Lexical Semantics: Meaning of word
        lemma(title), sense(different meaning within diff context), definition
        connections between words (synonyms, opposites, etc)
        taxonomy - absract(general words) to concrete(specific).
        Semantic frames and roles -> to denote perspective or participants in an event.
        
    Problems with discrete repr:
        one hot vector(give each word separete numbers on vectors for 1): sparce, subj, expensive, hard to guess relationships.
        goal: to map similar meaning workds closely, can use KNN to find close words. Standard way to repr mean ing in NLP.

    Training of word embeddings:
        Wrod2vec: 
                Cbow(opp of skip gram) - from many guess one.
                Skip Gram: use imput to geuss bunch of words (define a sliding window and train a classifier with padding, 
                            to predict correct token for all sectences. roughly self supervised.)
                            - some words used a lot, learn a lot of repr in language.
                            - others used less, more concrete, meaning very unique.
                            uses ONE HOT + embeddings to train/predict surrounding/similar word repr.
                            - loss function log probablity(dot product to predict similarity between surrounding words and 
                                    normalize with all words -> expensive so approx by negative samplinig(sampling without main 
                                    sliding window but vocab is not all unqiue - but can use freq for norm) idea grab random samples 
                                    to do sum of pairwise normalizaiton -> convert to binary classification-easier to optimize and 
                                    learn repr) then use some gradient decent to predict.
                            - evaluation: check quality: instrinsic eval- test repr align with intuition about word meaning, using cosine similarity of word embeddings.
                                        extrinsic eval: test weather repr good for downstream tasks, such as taggin, parsing, QA or some task used by some model.
    Text Classificaiton: generative vs discriminative.
            generative - prob joined with and ouput, argmax over joint prob.
                - prob of input first, probably more expensive.
            discriminative - prob output given the inout data, argmax over conditional prob.
                = directly models prob with output space.
                - BOW(bag of words) discriminative model, does not care about word order. get score to prob then gradient descent. but for neural networks can use sequential order.
                
            logistic regression good for binary classification. sigmoid for binary, softmax for muti-class to repr to valid probabilty.
            gradient descent - gradient of loss function and use chain rule. to move to direction that decreases the loss. lr is the speed of movement.

    Q: norm samlnig denom is some sort of avg to shorten error?
    Q: so do we use freq when norming or no? since Vocab is not unique?
    Q: how does input become smaller dim(PCA?)?

    HW1 classification example:
        Continous bag of words(CBOW) has lower dimension, just bag of words(BOW) is one hot size of each lookup is number 
        of vocab. + linear layers. then softmax function and optimize with gradient descent and regulizer.
# LECTURE 3 : Language Modeling
    Ngram Language model, Neural Model, Evaluation

    define: final end of sentence token is special. 
        problem is probability of grabbing sentence from 

    What can it do: score sentences(high/low), or generate sentences(prob: of next work/token)

    N-gram Language model:
        every word is independence, and can use maximum likelihood MLE to see how likely eaach word is supposed to appear(by counting).
        - allows prediction of whole sentence too.
        - want to score with unknown read words,  interpolations w/ UNK model enables to allow low frequency unknown words.

    higher order model: limit context length to smaller window(n-1). MLE is count and divide. linear interpolation to deal with zero counts.
        sometimes add one smoothing, every word adds 1 count.

        other smoothing techniques: additive, discounting, kneser-ney. most of them to address zero count issue.
    
    Limitations of Ngram:
        Ngram cannot share strength among similar words, need class based language models.
        cannot condition on context with intervening words. need skipgram language model.
        cannot handle long-distance dependencies, need cache,trigger, topic, syntatic moels, etc.

    Linear models:
        cant learn feature combinations, logical errors - cant be expressed by linear features, use neural language models.

    Neural ngram model(since proximating current based on previous, but using neural model to learn embeddings):
        convert word predition to discriminative text classification.

        feed-forwrad neural language models. - uses previous words, looks up word embeddings, combine linear vector to get intermediate 
        vector + bias to get score and then get probability to get next token.
            (grab two d dim vectors and grad a d vector absed on that, convert to v(vocab) dim, softmax to prob)
        
    
        Solves: simililar word/context issues, intervening words, cannot handle long-distance dependencies.

    neural networks allow design of complex functions.

    PROS ngram: models are extremely fast, better at modeling low frequency - TOOL: kenlm

    ngram lm(non parametric-non training just statistics), prefix context table and counts will be massive to store. to compute just retrieve row.

    neural counts but also compresses, memorizes and generizes.

    Non-parametric: • Memorize low-frequency pairs of (prefix, next word) • Cannot generalize to unseen pairs • Interpretable & easy for incremental updates (insert/delete/replace)
    Parametric: • Learning from data-driventraining • Good generalization tounseen pairs • Black-box & expensive forparameter updates withforgetting issues

    Why llms so powerful: 
        - Architecutre, MLP then RNN and now transformers to capture sectquecnes and effecienceis to scale to 100B+.
        - Pre-training on massive raw texts.
        - Fine-tuning on language intrustiosn with supervised learning or RLWF.

    LM Evaluation:
        - log prob of sentence over test corpus, aggregate them log likelihood score(not comparable to different test sets), so we normalize it by count, turn into per word cross entropy and grap perplexity.

        unknown words: 90% of words are low freq, cut size of corpus(by unk or rank threshold), treat 90% as unknown words, to save compute.
            - subword tokens can handle loss of accuracy by using sub tokens instead of throwing tokens away, also handles mispellings.
        
        IMP: cannot compare models over different vocab. 
            Or more accurately, all models must be able to generate the test set (it’s OK if they can generate more than the test set, but not less)
            • e.g. Comparing a character-based model to a word-based model is fair, but not vice-versa

# LECTURE 4: Recurrent Neural Network 

    General knowledge:
    - when working with minibatches. need to add artificial token to make sure each sentence has same length. add paddings. (PER BATCH ONLY)
        - when computing avg we should zero out padding tokens, sum out embeddings over word and divide by number of real tokens-non paded length.

    Sequential:
        long distance dependencies in language can be compicated, nouns, or references such as 'it'.
    
    Recurrent Neural Networks:
        Approaches:
        1. variable size inputs: add zero paddings and concatate + Linear layer -> simple but downt works for longer sequences. massive ff layer.
        
        2. feed one word to each feed forwawrd layer : for each token dot product with weight + bais, and pass to next token. 
                (idea compress and pass info to next token to carry on, avoids super long W matrix.)
                do not need to pad senteces, and each ff layer is smaller than previous method.
                maiin idea for RNN, we can have many layers as we want, variable depth network. remembers information along time dimention.
                we can predict per token vs label, grab loss and sum them up for total used in backprop. backprop updated weights(rnn) not inital hidden layer passed on.
                backprop over time sequence - issue vanishing/exloding gradients. if we do back prop since al weights are shared.
                    - handling vanishing gradient: LSTM - long short term memory. make additive connections between time steps. gate to contril info flow, and 2 mem cell to store short/long term info.
                        - forget gate-how much forget long time mem, input gate-how much to add to long term mem, compute new values to add to memory, output gate-how much to pass on from long term to short(next step).
                        - all is passed to next time step(short term - only some portion of long term-higher freq of updating, allows prediction per token in seq).
                    - handling exploding gradient: gradient clipping(general idea can be applied to others). we norm by some number before updating, basically making it take shorter steps, instead of massive ones in gradient descent.

                exposure bias/distributional shift: training we pass in ground truth per token, but during test we pass on predicted words to predent/finish sentence. since its a prob dist model might sample an unknown sequence.
                    - happens for generative tasks, not classification.
                    - to handle:some reinforcement learning or scheduled sampling. first train on ground truth for some iter but gradually switch to taking its own input, and always continue like that. 
                                        - cannot start with pass in its own input, or else it will learn garbage. start with supervised then reinforcement function.
                                        - handles model robustness to handle new sequences, not necessarily accuracy. 

                                        Q: do we have to write our own feedback func to say weather new seqs in training during exploration are good or not?
                                            - model is baked in to explore some similar but wrong words but should later still learn the proper sequence. so overall message is not lost. not necessarily RL but SL + noise.
                
                Applications:  represent a whole sentence to make pred, represent a context within a sentence(read context up intil that point).
                                - can do 1-1, 1-many, many-1, many-many, many to many fully conencted.
                                    image captioning, sentence classification, machine translation, sentance tagging.
                                
                                - can do binary or multi class prediction.sentence repr for retrival, sentence comparison, etc.
                                - sequence labeling, calculating reprs for parsing.
                                - language modeling - tagging task, each tag is next word. is a auto regressive model(curr depends on prev token.)

                                bidirectional rnns - to handle two directional languages, also llms use it, also useful for sentence repr.


                What can LSTMS learn: 
                ...

                Efficiency tricks: minibatching makes thigns faster. harder in rnns than feed forward networks. since each word depends on prev word. (better to separe padding and end of sentence token)
                                - using a masking token to zero out paddings. to exclude it when calc loss. and divide sum of loss by real tokens in minibatch.
                            
                            bucketing/sorting - put sentences of same length with same minibatch. but can result in decreased performance.
                
                handling longer sequences: cant fit massive lang in mem, truncated segments. separete it into 2 passes but connect end of 1st to start of 2nd's hidden layer.
                                            - backprop individually though.
                
                Gated Recurrent Units: LSTMS but way more efficient. with better parameters.
                architecture search - basic lstms pretty good(the various combinatoins gates searched.)

                Multi-Scale Pyramid RNNS: one for word, one on top for sentences, one for paragraphs.

# LECTURE 5: Attention and transformers
    Encoder - Decoder models: used to be popular, nowadays only decoder(encoder merged within). prob, compressing sentences of varying length to single vector, hard to decode later on.
        - idea use mulitple vectors bsed on length of sentence: attention.
    Attention: embed vocab to dict, query a target element, pick relevant sources by comparing query/key-value based on confidenc. summarize relevant work tokens into a contetxt vector, used to predict target token.
        encoder gives keys and vals to each word based on learned functions, decoder creates queries to check/find relevant key. we dont use argmax on keys (not differentiable(is discrete)) instead use softmax (to smooth attention dist), is differentiable, allow backprop.
        after grabbing query results, we grab norm attention weights, create a context vector with weights + base word vectors, and pass it though a FF network + predicting base word vectors to predict per label. 
            Feed Forward:
            - multi layer perceptron great for large data (applied position-wise after attention)

            Score (attention alignment):
            - dot product; if too large dim, use scaled dot product
            - bilinear (general form): score(q,k) = qᵀ W k
            - MLP/additive (Bahdanau): score(q,k) = vᵀ tanh(W_q q + W_k k)
    
    transformers: adv: really fast, only matix, parallel sequetial processning, efficeint.
        - self-attention - allwos parallel comp of all tokens.
            attends to itself most, but also givs attention to other words/elements in sentence, context sensitive.
            using embeddings to grab key,query,value. use querry to grab ino from each other embedding, use softmax on all. use values and softmax dist to grab sum. saically summarizes its own info and other words info. can stack multiple self-attention layer.
        - multi headed attentions - allows querying multiple positions at each layer.
            multiple attention heads focus on different parts of sentence(can encode in parallel).used several heads per set, t heads to create a single d(t) dim vector.
            add nonlinearity - FF(MLP), so output is no longer liner to input, add capacity to model to make it complex. we stack self-attention(multi head) and non-linear layer N times to increase capacity.
        - position encoding - addds position ino for each token.
            model perspective no position info, since parallel. add soe info function at the very beginning to augment input where word is location.
            naive = integer index and another dim to input. issues embedding vector also numeric, so may cause instability.
            we use frequency based functions, sigmoids with diff frequency. differnet dimentions say differnet info. 1st even odd indicator, layer first-half etc. each word have added dims. combination if some of those dims give positional info. typically concatate with word embedding info and pass them as same.
            another idea add learnable parameter for every input. but fixed size for length, need to estimate or model might not work. typically more optimal but more complex.
            typicall absolute not useful, relative to other words more useful.
        - adding non-linearrities - combiens features from self-attention layers.(expl above)
        - masked decoding - prevents attention lookups in the future tokens, to avoid predicting future tokens.
            avoids looking into future tokens, masks attention to future tokens(keys).(step before making softmax dist). future token will not affect context.

        self attnetion: combine words
        multi-head = learn independ head information
        norm dot prod atten - to remove baias in dot prod calc
        prod encoding - without RNN, to udnerstand location.

        Training tricks: layer norm - avoid number blowouts, training schedule - adjust from adam (study show needs some warmup from small to large then exp decay to find optimal lr), llabel smoothing - insert some uncertainty to training proc(allows sampling since finding a single point is rly hard).

        Attention hard - slow at test, doesnt necessarily outputform RNNs on decoder side for seq2seq taks (transformer need a lot more data), hard to train on small data.

        EXTRAS:
        Additions: can add markov properties i.e. add attention matrix. encourage sparcity: instead of softmax to smooth out, ask model to pick a few tokens instead each time.
            incorporating markov properties: attention from last time correlated with attention from this time. add attention from last for next deision.

            Hard attention: instead of softmax, use zero-one decision. harder to train, needs Reinforcement learning, perhaps helps interpretability.

        Better Training: 
            neural models tend to drop or repeat content, add penanty if attention not 1 over each word.

            attention is not alighment, often blurred, off by one on matrix for translation, and other tasks too.

            supervised - pretrained + supervised is strong alignment model. but tons of data, model can generally learn too.

        What else:
            Copy Mechanism: model can not only generate from vocab, but also copy tokens directly from input/history, final prob = mix of generate prob + copy prob

            hierarchical attentions: attention over words then attention over sentences then document. grab context of document.

            attention allows various modalities, vision, text, sound/freq, etc.

            multi source attention.

            basic idea of attention: idea similar to info retrival.

# LECTURE 6: Pre-traind Sentence and Contextualized Word Reprs.

    why pre-training, 

    non-Contextualized - word2vev, seq ,etc.
    contextualized - bert etc.

    pass non-context though an multi layer perceptron to turn into contexualized?

    why pre-trainng: learn multi-tatsk learning, transfer learning - finetuning

    NLP TASKS: text - language modeling, Natural occuring data - machine translation, hand-labeled data - most analysis tasks.
        - 1. Perform multi-tasking when one of your two tasks has greatly fewer data, 2. Perform multi-tasking when your tasks are related

    multi-task leaning, train reprs to do well of multiple tasks at once, i.e. tagging, language modeling.
        - should be table to train encoder on large dataset for one obj, then use same encoder+finetuning later for other tasks such as tagging.

    3 main objtives - define model, define training obj, data.

    end-to-end - is done if easy or has enough data points. else always use pre-training.

    Usage of sentence reprs: 
        Sentence Classifcation - sentiiment, classify, binary labels over sentence, etc.
        Paraphrase identification/Semantic Similarity - compare meaning semantically between phrases
        Textual Entailment - learn implications(Entailment), Contraditions, Neutral.

    Pretraining Techniques:
        (idea model pretrained for cetain task, then encoder/embeddings used from that to be used in another task)
        eg 1: Google: model: LSTM, obj: LM obj, data: Classification data itself or amazon reviews. downsteam on text classficaion, init weights and continue training.
        eg2: GPT: Masked self-attention(transformer), LM obj, Data: BooksCorpus. downstream some task finetuning other tasks additioanl multi-sentence training.
        eg3: Auto encoder + Transfer: LSTM, from single sentenve vec re-construct the sentence. Data: clasification data itself or amazon reviews. downstream on text classification init weights continue training.
        eg4: Skip-thought vectors(sentence level contet): lstm, predict surrounding sentences, data books, imp because of context. downstream sentence pair classification. 
                (Similar to Skip-gram that predict the surrounding words by the center words)
        eg5: paraphrase based contrastive learning: try several model, predict weather two phrase are paraphrases or not, data: paraphrase db from bilingual data.downstream: sentence similariy, classification, 
                (LSTMs work well on indomain but word averaging generalizes better.)

        deep-fusion - concat sentence for grabing self-attention(sentiment - close, deep,repr, efficient)
        late-fution - do encoding on ecah then gram.(sentence classification - sparse, more time?)

        eg6: Entailment + Transer:Bi-LSTM + max pooling, (use human labels for pretraining) supervised training for a task such as entailment learn generalizable embeddings. Data: Stanford NLI, MultiNLI, 
                (Tends to be better than unsupervised objectives such as SkipThought)

    Context word repr:
        Central Word Prediction:
            context2vec: Bi-directional LSTM , Predict the word given context, data: 2B word ukWaC corpus. downsteam  use vectors for sentence completion, word sense disambiguation, etc
            ELMo: Model: Multi-layer bi-directional LSTM • Objective: Predict the next word left- >right, next word right->left independently 
            Data: 1B word benchmark LM dataset. Downstream: Finetune the weights of the linear combination of layers on the downstream task

    Masked Word Prediction:
        Masked Word Prediction (BERT):model: Multi-layer self-attention. Input sentence or pair, w/ [CLS] token, subword representation, obj: Masked word prediction(mask certain words ask predict) + nextsentence prediction((are they close or far))
            Data: BooksCorpus + English Wikipedia (16GB)
        
            select 15% of words randoly, replace with mask 80%. time, 10% replace random(to simpulate noise), 10 no change. predict all maskd words given surrounding context. like context2vec but better suited for multi-layer self attention.

        Consecutivee Sentence Prediction: classify two sentence as concsec or not. data = 50% of trainngin from openbooks is consecutive, use unique cls token to include meaingn of two sentence. mask some sentece in sentence and sep tokens to separaee sentences.  and binary predictions.

        Hyperparameter Optimization/Data(RoBERTa) - same as BERT, but trainlonger and drop sentence prediction objective. but with much larger data. empirically much better.
            - sentence prediction is easy(doesnt help) if mask token prediction is done well(hard-loss goes down slow).

        Distribution Discrimination -ELECTRA: same as bert,Sample words from language model, try to discriminate which words are sampled. discriminator used as word encoding model later on.
            (masked sentence passed in generator, which predicts/sample words, discriminator then predicts if original word passed or not.) 
            (better than bert, more gradient updates to model since per token labeled, bert only does 50% as their tasks masks 50% only)

        Permutation-based Auto-regressive Model+ Long Context(XL-Net):Same as BERT, but include longer context, Predict words in order(itself and before), but different order every time(train sentences with different permutation orders: some noise but able to learn differnt perspectives). Data: 39B tokens from Books, Wikipedia and Web 
            - later version, instead of shuffing randomly, it group phrases to avoid random places, avoid learning wrong word combinations in sentence.
        
    Compact Pre-trained Models: 
        ALBERT (Lan et al. 2019): Smaller embeddings,and parameter sharing across all layers, but the same inference time as the BERT counterpart 
        DistilBERT (Sanh et al. 2019): Train a model to match the distribution of regular BERT
   
    SUMMARIES:
        Which model:
            - Averaging word embeddings can outperform more complex models, especially on out-of-domain data.
            - BERT shows the strength of bidirectional transformers but does not benchmark against LSTM models like ELMo due to lSTMs being constrained by computational efficiency and scalability.
            - With BERT-like training data, their ablations show architecture and training tweaks can yield further improvements.

        Which Training Objective? chatgpt
            - When using the same data, bidirectional language modeling outperforms a machine translation encoder.
            - Next-sentence prediction, combined with masked LM, seemed helpful in the original BERT paper(boosting perf), but later work (e.g., RoBERTa) found it unnecessary and even limiting when scaling training.
            - Next-sentence prediction provides little to no benefit and can be dropped.

        Which Data?
            - Preliminary results suggest more data generally improves performance.
            - Scaling with large web data yields gains but inconsistently.
                Data with context is probably essential/best.

        Pre-trained Large Language Models: 
            GPT-3: similar to GPT-2, but pre-trained on lot more data using autoregressive LM objective. Demonstrate good few-shot in-context prediction ability.
            ChatGPT: init form GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF).

# LECTURE 7:Parameter-efficient Fine-tuning - and other ways to bypass costly fine-training.
    PEFT - Fine tune a small amount of mdoel params, instead of entire model. on a small dataset of downstream tasks. Other parameters are frozen.
        pros - reduce compute and sotrage costs, 
            mitigate catasprohic forgetting
            easy to update model to new data and facts
            better perfm on low data
            comparable perom to finetuning
    
                                PEFT                    | Full Fine-tuning
    Learnable parameters:         A small subset        | Entire model
    Training Performance:         Close to fine-tuning  | …is fine-tuning :P
    Training Data:         Small                        | Large
    Training Time:         Faster                       | Longer training time
    Overfitting / forgetting: Less prone to overfitting | More prone to overfitting

    3 ways to adapt model capacity(Three Computation Functions): 
        function composition - add extra functions (combime multile as well) on top on NLP layer, (new function layer)
            - i.e. Adapters - additional modeluse in layer - match or outperfms fine tunign.
        input comp - combine data as input to create longer sequence. has effect on self-attention layer. (adapt data)
            - i.e. prompt tuning, prefix tuning (beginning of sentence instead of end)- context window of model is increases - good for large models
        parameter comp - low rank extra params in middle of the model, in matrix math area to increase wight by adapter wight. (adapt model with addl func)
            - i.e. Lora/QLora - no increase in model size, good.

    Function Composition:
        Adapter - adapter layer after each FF layer (before layer norm if there). Adapter itself is an multiplayer -2 layer perception.
            - each layer is a matrix which transforms the input in new space. adjust embedding direction. helsp reroute the data embeddings to what the "upper layer expects."
            - increase capacity of model to learn new date.
            - vs. full finetunign. small data better prfm with adapter, and about equal perfm with large data. better accury with fewer num of params updated. embeddings repr shifts little(origial meaning holds?) vs finetuning.
        
        IA^3: Add learnable params to self-attention layer instead on NLP, - model selects params that are moe and less imp for given downstream task.
            - adds learnable layer to MLP layer too.

    Input Composition:
        Prompting with text: prepending instructive words before actula test input. 
            - text tokens as learnabale params.
            - trigger mdoeling to learn tasks- 
            - models are sensitive to the formulation of the prompt and to the order of examples
            - Prompt Tuning: 
            Prompt tuning: only updates a small task-specific prompt parameters(learned, i.e. extra params added to left of embedding) for each task, enables mixed task inference.
                - during testing, add learned prompt to input.
                - as model size increases the perf of prompt tunign matching full fine-tuning.
                    - naivly few shotting model is still wrose than both.
                - only updates prompts, not x inputs concated.
            Prefix Tuning - more effective to capture gradient updates over layers.
                - adds larnable params at the beggingin on the input sequence of all layers. (multi layer )
                - better in low data regiuemes, better in general too - full finetunign.
                - only prefix updated, rest model frozen.
    
        Q: inputs trained to or just added to front.
            - no inputs trained, empty/learnable parameters added to front after capturing sentence embedding
        Q; prompts multiple voab or just 1
            - 1 or multiple your choice? More training?
        Q: Cost of adding adapters and input prompting compute costs?
            - there is added costs/params to both methods, but negligible.
        Q: Combination of these better in some instances or typically by itself better.
            - not necessarily, main idea is to find the sensitive spots and adjust them. There is a paper that is trying to find the spots and test multiple methods/combination to see if any better. Auto smth?
        Q: does fine tuning increase inferece speeds compared to original foundation model?
            - not inference speed, just training speed.
        Q: Does lora have any impact on forgetting? since we are adding to params? 
            - if matrix conflicts with other for the updates, depends on tasks if the knowledge confict w/ pre-tain knowledge. All methods can have forgetting no gurantee/non-gurantee.
        Q: Qlora explanation - quantization has some lsoss, previous does it affect performance? what dooes the bits reprepsent.
            - bits represent weights, we shink the specificity of the weights to a smaller "scientific number", not massive loss in performance if quantization done properly.
    Parameter Composition:
        Low Rank Adaptation (LORA)- approx self attenupdat of lernable weight by a low -rank matrix. initi update is 0, after training updates are added 
                back to original checkpoint. so at inference cost of the updated checkpoint is samea s orig checkpoint.
            -   save a lot of params, no actual overhead in inference. small overhead to do computation during inference.

        LoRa works better than other PEFTs (adapters, full finetunign, - prelayer, etc).
        why works: full tuning update typically happen in some low rank space, so we can try to update with low ranks as well.
        pretraining provides strong initization in D dimm space, model only needs to explore subspace during fine-tuning to learn final weights.
        Reduces VRAm consuption, fewer gpus and fewer I/O operations.
        lora still reqs forward comp and back prop. so lora speeds up 25% but noto 10x compared to fine-tuning
        
        QLORA - Convert information in a high-precision data type to a low-precision data type, Allows training a LLM in a single consumer GPU 33B in a single 24GB gpu.
            - lower precision avg. nowdadays. use fload 8 or mixed precision. - typically dont need very accurate steam for numbers.
            takes 32 bits to some rounded 8 bit representations, then after gradient updating on smaller bit dequant it back to fp32 representations.
            Double Quant: quantize the fp32 constant inside the fp32 bit, to further save memory on top of quantizing weight matrix blocks. dequant will have an extra nested step as well.
            quantization hsa some information loss, so we do blockwise quantization after flattening matrix(to avoid the max working on a large matrix), to minimize loss.
                - more sensitive to Hyperparameter, perm not same as full-finetunign due to agressive compression.
            can match certain fine-tuning methods with good methods.

        Check picture grpah to get and idea of eah model acc vs num of param.
            - depends on usecase which method better than others.

        Other ways to bypass expensive fine-tuning:
            - PEFT is one way to adapt model to downsteam taks
            - prompting incontext learning - doesnt require furthier training.
            - Retrieval Augment Generation(RAG) to steer model perfm - need to update model to use actual data.
            - Model editing - tuning free approach to modify behavior
                - computationally innexpensive, targetted modification to params. - if we understand causal mechanisms on internal layer
                - cons - kinda like blackbox. hard to understand?
                - Editing knowledge editing - knowledge store or update weights(locate and edit, constrained tuning)
                    behavior - inference time , activation editing, decoding time.
            
        Training models are sene as vectors: has additive propertives, skills embeeded in linear subspace, can combine to explore anothter trajectoty.
            - can take negation of task to unlearn, 
                - Can reinforce certan skills, or reduce certain skills. with combinatiosns (+ then (-), etc.)
        
        can steer model on self atten layer, modify attention heads. need good understandign of heads to steer or remove heads.
            - (incase its persay learning harmful info/hateful speech) can do targetted ediitng as well.
            - targetting heading better than fine tuning and other approaches as well.

# LECTURE 8: Instruction Tuning and Multitask Learning
    several tasks in nlp, only text-for language modeling.Naturally occurring data: e.g. machine translation. Hand-labeled data: e.g. most analysis tasks, 
    multitask: we can train models on many data to do well on several tasks.
    pretrain finetune: we can pretrain on one task and then finetune for later.
    prompting: Train on LM task, make predictions in textualized tasks.
    instruction tuning: Pre-train, then fine-tune on many different tasks, with an instruction specifying the task.
    
    Basic Fine Tuning: Build a model that is good at performing a single task
    Instruction Tuning: Build a generalist model that is good at many tasks

    Examples to build/evaluate machine understanding instead of just memorization.
        give context free question answering dataset/open-book QA, answer a prediction wihtout any specific groundnig knowlegde.
        give contextual question answering/machine reading/closed booking qa: Answer a question about a document or document collection
        Code generations: generate code from natural language command.
        Summarization: single doc-compress longer doc to shorter, multidoc-compress multiple to one.
        Info extraction: entity recog - identify which words are entities. Entity Linking - connect entity mentioned to another knowledge bases. 
            Entity coRef - find which entity in input correcpond to each other(him mean obama, etc.). Event Regoc/linking/coRed - identify what events occurred.
        Translation: from one lanuage to another. quality assessment done using similarity to ref translation.
        General Purpose Benchmarks - collection of general NLP tasks of past decade used to benchmark. to guage ability across broad range of tasks.

    MultiTask Learning: Perform multi-tasking when one of your two tasks has fewer data : High-resourced language → low-resourced language, General domain → specific domain.

    Domain - one task but data might be from several areas. sometimes labeled other times now.
        in practice: Content, what is being discussed.Style, the way in which it is being discussed. Labeling Standards, the way that the same data is labeled

        Types of shift: Covariate Shift: The input changes but not the labeling. Label Shift: The output dist diff (which also implies the input changes). Concept Shift: The conditional distribution of labels changes (e.g. different labeling standards).

        Generalization: 
            Domain adaptiation-train on many adapt to a target domain at testing. 
                Supervised adaptation: train w/ target-domain labeled data. Unsupervised adaptation: train w/o target-domain labeled data
            Domain robustness-train on many doms perform well on all doms.
                esp minority doms.Zero-shot robustness to domains not in training data.
        Detection: Binary classification: detect whether a test example is an OOD example or not.
        
        Multilingual learning also a multi task learning.

        i.e.: Languages as Domains: Multilingual learning is an extreme variety, different language = different domain
            Adaptation: Improve accuracy on lower-resource languages by transferring knowledge from higher-resource languages 
            Robustness: Use one model for all languages, instead of one for each

        Earlier Method on Multitask Learning Feature Space Regularization: Try to regularize the features spaces learned to be closer to each-other.
            encourage rep learning. classifier to learn class label on top of adversarial feature extractor for downstream tasks. from end of featrue extractior we also put domain classifier to encourage((to check)) domain specific learning(and to avoiding mushing everything/all domain in same space).
            idea: want the encoder to learn representations useful for the task. But also want the encoder not to “overfit” to one domain if you want transfer/invariance
                label classsifier minimizes loss from both feature extractor and label classifier to make feature predictions on downstream labels.
                domain classifier minimizes loss on bomain classifier but maximiszes loss on feature side, to encourage features that are hard for domain classifer to distinguish - domain invariant.
                    i think the idea is if the feature extractor learns representations that confuse the domain classifier, then those features must capture what’s common across domains, making them more general and transferable.
                Encoder basically minimizing label loss while maximizing domain loss so those features hide domain cues and become domain-invariant.
            if domain specifc invariance is good, can be confident label classifier will also be able to generalize better.
            neding underlying similarity to pass down task - cant work in unbalanced, un related, etc. can have neg transfer-degrade perf. need underlying assumption of some similarity/structure to enable transfer. 

        Simpler Multi-task Learning Method(data manipulation): i.e.Adding Domain Tags: add tags to indicate what domain or language let model know aht it would want it to do later. Model still doing task prediciton. introduces a small number of params.

    Instruction Tuning:
        T5: Text-to-Text Transfer Transformer -Supervised training on many NLP tasks, Control a single model to do tasks following the instructions in the prompt.
        T0 model (Sanh et al 2021) from Hugging Face& google: Multitask Prompted Training Enables Zero-shot Task Generalization. i.e. trained on summarization, sentiment, question answering but was able to do inference on natural language inferece
            can improve zero-shot perfomrnace than gpt zeroshot and gpt fewshot.
            FLAN Collection: instruction tuning dataset.
            models ex: Flan T5, LLaMa2 Chat, Mixtral Instruct

    Synthetic Data generations: 
        Self-Instruct: to generate syn data and train model again.
            Models sees all tasks, generates synthetic tasks instructions, then ask model to follow isntruction some classifications, and create outputs(ouptut first/input first), then filtered ans passed back in pool again.
                i.e.Input-first: Ask an LLM to come up with the input fields first based on the instruction, then produce the output. (or can use some label and then ask to generate text based on that label i.e. sentiment, etc)
                    Output-first: Generate the output label, and then generate the input based on the output.
    
        Evol-Instruct:  idea of evolve instruction into more complex instructions.
            filter out invalid instruction, find similar instruction but not smae intent, etc.
                indepth - you can rewrite given prompt into more complex version to make tasks more challengin even though same intent.
                in-breadth evolving - draw inspiration and create a completely new prompt same domain but even more rare.
            used to create new instruction pool to increase reasoning examples, deepening, complicate, concretizie, add constraints, etc.

    Knowledge distillination - large model to small model to run deployable env.

# LECTURE 9: Prompting and Zero-/Few-shot Learning
    Four Paradigms of NLP Technical Development
        ■ Feature Engineering, popular around 2015
            fully supervised learning, neural network based models to train classifier.
            manual features - svm, crf.
        ■ Architecture Engineering - learn feature than hand design feature 2013-2018
            lstms, cnn, gru, etc. dont need to manually define features. modify model architecture.
        ■ Objective Engineering - pretrain, fine tune , 2017-now. self supervised
            bert -> finetuning. less work on architecture but engineer objective functions. self training w contrastive learning,etc.
            still need to init full model and add shallow layer on top.
        ■ Prompt Engineering - pre-train, prompt, predict, 2019-Now.
            engineer prompts to predict in zero shot or a few shot.
            chatgpt, 3,4.

    Prompting: encouraging a pre-trained model to make particular predictions by giving a prompt specifying the tast to be done.
        general workflow: prompt addition, answer prediction, answer-label mapping.

        prompt addition: input and template with x that model predicts, then map predicted x as specific sentiment for example positive.
            cloze prompt  - predict x like before.(w llm)
            prefix prompt - x token placed at end, (use autoregressive movel to predict)

        Design Considerations for Prompting:
            (look at Fig 2: image for considerations)
            ■ Pre-trained Model Choice
                popular choises: 
                    (Left-to-Right) Autoregressive LM (useful for larger scale lms, counting based, good for language generation, works well w prefix prompt, use at test time, no need to train - i.e. GPT 1, 2,3,4),
                    Masked LM: bidirectional, great for NLU tasks. Bert,Roberta, etc. use Cloze prompt, language understanding.,
                    Prefix LM: combination of mased and autoregressive, prefix prompt, Corruption operations can be introduced when encoding X,
                    Encoder-decoder LM: A denoised auto-encode, very similar to prefix lm, 2 transformer model and two different masking. bart, t5.
                        MASS: transformer based - only predict masked spans 
                        BART: Transformer-based - Re-construct (corrupted) original sentences
                        mBART: Transformer-based Multi-lingual Denoising auto-encoder - Re-construct (corrupted) original sentences 
                        UNiLM: Prefix LM, left-to-right LM, Masked LM  - three types of LMs, shared parameters
                        T5: left-to-right LM, Prefixed LM, encoder-decoder - use instruction to explore different downstream objectives respectively(modified data format, to model learns to do multi-task instruction following)
                        Application of Prefix LM/Encoder-Decoders in Prompting:
                                Conditional Text Generation: □ Translation □ Text Summarization,
                                Generation-like Tasks: □ Information Extraction □ Question Answering
            ■ Prompt Template Engineering
                traditionally take input and predict prob in label space. but prompt formulation deinfe input, template and predict z in template. and map predicted token to label(but how to deisgn suitable prompt).
                    prompt shape: cloze prompt, prefix prompt(more popular)
                    prompt template: hand crafted work, automated searches - find optimal objective function, in discrete space, or Continous space(i.e.prompt tunign- learn prompt exameple.)
                        prompt search:
                            prompt mining: early work, find prompts given a set of question/answers: dependency parsing to capture sentence structure(i.e. dependency tree), 
                            Prompt Paraphrasing: paraphrase an exiting prompt to get other candidates - i.e. back translation with beam search (bidirectional translation models, translate and untranslate and try differences to get best output), 
                            Gradient-based Search: i.e. Autoprompt. have tarining example but dont know which prompt will work best. find prompt tokens that maximize performance.
                                Q: (confused.)
                                    basic idea is to automatically find “trigger tokens” (words or short phrases) that, when added to an input sentence, make a pretrained masked language model 
                                    (like BERT or RoBERTa) behave as if it was fine-tuned for a downstream task (like sentiment classification).
                                    Autopromt searching for a few words(tokens) that manipulate model into behaving like a classifier. [input][T][T][T][T][MASK]
                                    after it learns from a few examples, can be implemented into new input examples, to add tokens to make main model predict the masks, the prediction sentiment can be used similar to classifiers.
                            Prompt/Prefix Tuning: obpitmize embeddings of a prompt, add embedddings later as prfix for at testing.

            ■ Answer Engineering
                We have reformulated the task! We also should re-define the “ground truth labels”
                map vocab in answer space to downstream calssification label space. if we can learn maping, we can include mapping in prompt, or post processing(extract & maping leter.) step.
                    Shape: token(single word), span(chunk of texts,usually used in Cloze prompt), sentence(supports structured output like xml, json, used w/ prefix prompt (seq2seq LM for generative tasks)).
                        token,span great for classifcation, sentence great for generation tasks.
                    Human effort: hand crafted(infine space - summarization, machine translation - map predicted tokens to final answer. finite space - text classification, sequence labeling - map finite set of words to labels), automated(discrete - Answer Paraphrasing, Prune-then-Search, Label Decomposition. continous).

                Chain of thruoght prompting - (still linear just chained) Instead of searching for the answer directly, and manually add some intermediate reasoning steps in the prompt to guide the model derive the answer
                    can improve performacne for deduction realteed tasks(i.e. maths). random generation to find path.
                    another version self consistency with COT - multiple COTs and majority vote.
                Tree of thought - Instead of search the answer using a linear chain structure, prompt the output sequence to follow a tree structure
                    still need decoding process to handle performance. if certain branches thought says not possible(validation process, with LM) we prune that branch. allows backtracking
                    can have false negatives - evaluator(LM) might make mistakes. might get stuck in loop(hand design termination), search space too large - slow and challenges api/LM/evaluator calls(too many).
                Graph of Thought: can self look over node, back track, and can aggregate chains to single node. single output(divide-conquer,sorting, etc.).
            ■ Expanding the Paradigm
                Multi-Prompt Learning:
                    Prompt Ensembling:using multiple unanswered prompts for an input at inference time to make predictions . Stabalizing performance, Alleviate the cost of prompt engineering(typical methods:unifrom averaging, weighted averagin, majority voting).
                    Self-Consistency Prompting: ask model to sample diverse reasoning tasks, dif temps & other Hyperparameters during decoding and check final answer to see which answer is dominant.
                    Prompt Augmentation: add examples ot help model understand what objective is.
            ■ Prompt-based Training Strategies:
                Data Perspective: How many training samples are used?
                    zeroshot- without explit training of LM for downstream
                    few shot -  few training samples
                    Full-data -  lots of training samples
                Parameter Perspective: Whether/How are parameters updated?(examples of strategies)
                    (Strategy: parms details - model ex.)
                    Promptless FineTuning: lm params tuned - BERT Fine-tuning
                    Tuning-free Prompting:  GPT 3
                    Fixed-LM Prompt Tuning: Additional Prompt Params + Prompt Params Tuned - Prefix Tuning
                    Fixed-prompt LM Tuning: LM Params Tuned - PET
                    Prompt+LM Fine-tuning: LM Params Tuned + Additional Prompt Params + Prompt Params Tuned - PADA

            General Guideline: 
            If you have lots of training samples?:Promptless Fine-tuning, Fixed-prompt Tuning, Prompt+LM Fine-tuning
            If you have a huge pre-trained language model: Tuning-free Prompting
            If you have few training samples?: Fixed-LM Prompt Tuning, Tuning-free Prompting

        
# LECTURE 10: Learning from Human Feedback 1
    POlicy gradient fundamentals - Reinforce, Policy optimization PPO RLHF standards, etc.

    Great Challenge of LLMS: pretrained llms are great for next token predictors, but not helpful assistants.
        behavioral gaps: factual incorrect, toxic or biased, off topic, not aligned with user intent. Goal birdge the gap.

        Reinforcement learning w/ Human Feedback, train lmms to align model outputs with humans preferences/intent/values(environment). (OpenAI did this)
            pivotal component  in success of modern instruction tuned llms, instructGPT< ChatGPT 4, Claude, Llama 2/3, deepseek math.
        
        pariwise preference feedback: ask user to choose between two responces, i.e. Chatgpt.
            RLHF applications - Helpful& Harmless, Toxicity and bias reduction, ethical alignment, Political neutrality. 
                - personalization, persona, roleplaying, Cultural & Linguistic alignment(politeness, user style respones i.e. laymans vs professional), domain specific adaptation(align w/ domain experts law/medical/etc.)
                - resoning - prefer more failthful resoning chain, planning optimize long term task success AUtoGPT style agents with RLHF loops.   
                LLMS as chatbot - without environment reward are stiill hard to get meaningful responses during Human comp interaction. Challenging to simulate an evironment in an online setting.
                    - InstructGPT-earlier version of GPT, superfized finetunign + reward dataset, then RLHF to make model.
                        dataset - plain: ask labellers to come up with arbitary task with diversity, few-shot: ask labelers to come up with Q|R pairs of instructions. User absed: a number of use cases they asked labellers to come up with querys for these data sets.
                            - 3 datasets: SFT data set ask labelers to write resp to given prompt. RM data: labelers give ranking of model output used to train the reward model. PPO data: data collected during RLHF without human labels, using RM to provide rewards. all colelcted for replay purpose.
                            SFT - pretrained gpt3 model on finetune model to instructure and finetune. cross entropy to forcemodel to follow trajectory.
                            RM:  collect comparison data and train a RM. smaller lM to rank K respones to  a prompt, pairwise comparison, pariwise ranking loss used.
                            3rd step RL: train RL policy model initialized from SFT policy model(continue from checkpoint end of SFT), using PPO algo(stable RL model), maximize rewards and Kl penalty from SFT model.(checkpoint from RL close to SFT checkpoitns to not forget instruction following)+LM pretraining on public NLP datasets
                            pretty much 3 steps used to train earlier chatgpt models.

        Reinforcement learning:
        REINFORCE - montecarlo version of policy gradient.
            Markov Dcecision process(during training) - LLMas agent(looks at state), action is reponse based on policy, environment is human, and feedback is given by state(query+resp) and rewards, 
                - state updaed as conversaion goes on. process repeats forming a trajectory(episode) of horizon t(generated long seq of dialogue history). reward uses sum of trajectory * discout factor. has artificial ending state, predefined - no rewards used ended (after deployment can sitll be adjusted to run forever).
                MDP has 5 tuple - state space(all dialogue), action space(all model can generate), environment transition(prob of moving to new state after perforing action in state),
                    Reward func(immediate expected reward after taking action in s), Discount factor(how much future rewards are valued with respect to immediate rewards)
            policy is LM and environment transition(user-unpredictable/unmodelable) is independent of policy - after llm responds or user follows up.
                goal is to maximize the expected return under policy to update gradeint by gradient ascent. hard to first compute rewards then compute gradient - expensive.
                    -log derivative trick - has additive property, compute gradietn in tackable way.
                    policy gradient - include gradient of polity at time t and weight it bu total reward of full trajectory tau.
                    gradient written with - Reward to go: using full reward for each time step causes high variance. by caulalty action cannot affect past rewards, so we replace full reward by total return from time t till end of episode. 
                        using exponentially Discounted reward to put more emphasis on recent times, and less towards future(which are less concreque), math wise discount reward would be bounded for stability, and to define markov recursion or is by definition.
                        basic idea of REINFORCE.
                    adding base line to reduce variable without bias. cumulative reward used as baseline to compute advantage(reward-baseline) to ground responses to a baseline(for easy vs hard tasks).
                        weight our likelihood by Advantage. adding baseline does not affect bias for expectation. does not introduce bias but cna reduce variance.
                        Designing baseline - predict final reward to predict current state(need to learn baseline function given params): can be done sentence level one baseline per sentence, or Decoder state level - one baseline per output action. Option 2: mena of rewards in minibatch - used in 1990, but GRPO Recurrent too(given prompt sample several response, and use mean of all rewards in mini batch).
                            Actor-Critic - another common choice using state value function yielding the advanctage, - critic model is trained to estimate state value function PPO uses AC formulation.
                        Want to sample multiple episode under current policy and compte avg gradient for more stable accumulative reward.then update policy by SGD, since we are working in mini-batches.

                    Practical tewaks - REINFORCE is simple and unbiased but often high variance and sample inefficient, 
                        increase batch size, norm, entropy and regularization are important hyeper params, LR and baseline quality too.
                        using bootstrapping(Actor-critic), baselines, reward-to-go, advantage estimators, and variance reduction techniques greatly improves performance
                    
# LECTURE 11: Project Pitch Presentations

# LECTURE 12: Learning from Human Feedback - aligning llms with human intent and valies. (continued)

    Proximal Policy Optimization(PPO)
        PPO uses actor critic -  a more stable variant of policy gradient
        idea is to replace accumilative reward with advantage. expected advantage weighted by policy. instead of fixed function we use a value paramerized func.
        A critic (value function) estimates the value function that quantifies the expected reward (scalar) for a given state
        A actor (policy) estimates the probability of an action given a state
        Why? - REINFORCE is high-variance and unstable even with actor-critic, when we take large gradient step it can cause perfm forgetting.
        a trust region: “Don’t move the policy too far in one update.” This leads to a thread of methods called Trust Region Policy Optimization (TRPO)
            PPO is a simpler, practical version of TRPO that constrains updates to avoid drastic policy changes.
            replaces he hard KL constraint with clipped surrogate objective. when optimizing expected advantage weihgted by ratio, we would containt ratio within a range.
                if smaller or larger, then model is moving too far away, we clip it to a bound. with epsilon Hyperparameter, typically 0-1.
        Full PPO obj - Policy loss(clipped surrogate) - value function loss(sq error) + entropy bonus for exploration.
        cliping ensures stability and avoid large steps, simple no second order approx, rertains sample efficiency on policy methods, 
            the advantage baseline keeps the gradient unbiased while reducing variance

    Group relative policy optimization (GRPO)
        PPO having a crit doubles memory/compute and value estimation may introduce variance or error.
        GRPO removes need for separate critic by estimating a baseline using group samipiiling - generate multiple outpits per prompt
            and compare relative to each other(uses mean and var of multiple rewards to normalize multiple advantage)
            [comparison image shared Fig3.]
        GRPO uses group normalization of rewards to define advantages across the outputs in one group: ONE IS GIVE SINGLE REWARD PER OUTPUT(OUTCOME SUPERVISION), or GIVE REWARD FOR EACH PROCESS STEP OF AN OUTPUT(PROCESS SUPERVISION)
            OUTCOME SUPERVISION: compute group mean and std, each output define norm reward, that is used a advantage for every token in ouput. (method just an approximation, does not need to be accurate)
            PROCESS SUPERVISION: rewards at each reaosning step. (can be 1 token or several grouped as reasonign step), rest same as before but applied to steps. can give reward middle of environment/sequence instead of only at end.

        By normalizing within each group, GRPO encourages relative improvement rather than absolute. Even if all outputs are “bad,” it still picks the least bad ones to promote.
            It reduces variance and avoids needing a value critic. 
            challenge (discussed in follow-up works) is when all sampled outputs are equally bad (e.g. all wrong) — then the group normalization gives zero advantage and no gradient signal (an “all-negative group” problem) 
            Summary:
                removed critic - no need to estimate value function.(benefit, robust across group)
                still needs reward function even though no reward model.
                uses mean rewards to estimate advantage.
    
    Direct Preference Optimization (DPO)
        removes the need to train a separate reward model and then run RL. Instead, it directly learns from human preference comparisons (pairs of chosen vs rejected) in a supervised-style optimization.
        offline Rl algo, used fixed LM model to generate output use humans to collect pariwrise pref data. This dataset is used to do DPO training.
            (online for example, LM model is always updated with rewards from pretrained reward model.)
            ask human to collect a lot of data, and use that to update policy without needing a reward model.
            Key takeaway: no explicit reward model, no RL loop, no value critic — it’s just a pairwise preference-based update.
            much more computationally efficient.

            Pros:
                Offline - simpler  (no reward model training, no RL iterations, training policy model learning impricitly)
                More stable (fewer moving parts).
                Empirically competitive with PPO-based RLHF in many tasks 
            Cons:
                Assumes pairwise preference data (cannot trivially incorporate more general feedback)
                Might be less flexible in incorporating more complex reward structures
                
    [High level comparison for RLHF shared FIG4.]

    Research on field here:
        Preference learning rewards modeling for value alignment, and reward for reasoning models as well.
        research going on to combine direct pref methods with Rl based.
        scalability and efficiency
        multi object alighnemtn, safety 
        open benchmarks, eval.
        theory & convergence anlysiis

        Pluralistic Preference Learning: human pref diverse and distributional, but llms are skewed to one TRUE direction.
            Train reward with conflicting reward, where preference is switching, DPO skews towards majority pref instead.
            Proposed distributional pref optimization GDPO, use distribution to calibrate model to not be monotonic. 
                uses belief ratio of pref used to weight in on proposed, collected when collecting pref data as well.


# LECTURE 13: Modeling Long Sequences 

    Document Level Neural language modeling - RNN and transformer.
        some language tasks like tagging, classification, parsing, 
        we want to rpedict prob in long documents, 
        1. Truncated Backpropagation Through Time(2011): do rnns in chunks, but state only no backprop beyond per state.
        2. Separate Encoding for coarse grained doc context(2001): add summary to each RNN state which holgs glocal features, of doc. 
            do topic modeling distribution, to thow away long context and grab main ideas to include.  
                prev works used bag of words to compress long to bag of words.
                other work used last hidden state.
        3. hierchical attention netowrk(2018):
            run world level RNNs then use inputs to run a sentence level RNN. Typically bidirectional BIGRU.
        4. Tranformers across sectensences:
            self attention can simply doc level context, might need to compute massive quadratic matrix. constraints on gpu memory.
            can learn interesting phenomena i.e. co-reference.
        5: Encode Context and Source Separately(2018) - use two tranformers one encode context and other current source. Share N-1 layers for two encoders.
            Context - prev/next sentence or random sentence in doc. subsampling to compress seq length of context, and share params in encoder. have special layer transform layer to entegrate context with source to make prediction(to be error tolerant).
        6: Tranformer-XL(2019): reuse Transformer arch, but allow attend to context from prev sentence.(standard encodes each chunk separately)
            prev chunks hidden info cached and passed on when encoding next chunk. but cannot backprop though chunks, only within chunks to avoid memory issues.
            how far can you look back? count of layers(N) times chunk size(L). (beyond just past chunk-can look really far - theoreticlaly O(NxL))
            7: Compress prev states(2019): extention of TransformerXL, instead of holding one memory they have an extra compressed memory where the cached mamory is added as strided compression(step over prev states).
                model can attend compressed and uncompressed to access even longer info.
        8. Sparse Transformation(2019): add stride, only attending to every n prev states.
            attenntion over only yhe cls tokens, skip other word tokens in sentnece. a lot info is compressed in cls token so dur attention for prev context, 
                can just attend to cls(beginnign token). still allow full-self attention for curr attention but for past.
            a lot of sparsity, can help save computation, another work found: if model has certain bias to attention, let model bias to wards those and fix n, and save comp as well.
        9.Adaptive Span Transformers(2019): some might have attention head only few token, others might spread it widely. utilize it to add sparsity differently in different heads to save computation costs.
        10. Reformer: Efficient Adaptively Sparse Attention(2020):  efficient calculation of sparse attention through
            found some tokens have similarity(hashing technique to group similar), sort them put then into buckets and but add to buckets and add a 1 of prev to next bucket(if you want to pass info to next bucket), 
                and do cross attention within and between buckets, the 1 in other bucket will hold info of prev bucket. helps locality sensitive hasing to efficient calculate high scoring attention wights.
                chunking to make sparse compu more GPU friendly
            no need to maintain token order, position embedding added. 

        11.Low-rank approx: Calculating the attention matrix is expensive, can it be predicted with a low-rank matrix?Linformer: Add low-rank linear projections into model (Wang et al. 2020).
            Nystromformer: Approximate using the Nystrom method, sampling "landmark" points(2021)
        
        Summary of Training-time Methods: current bottleneck of Transformer-based model for long sequences is the computation of attention matrix
            summ - attend to past memory, sparse attention, low-rank approx.
        
        Long-Context Language Modeling at Inference: During pre-training, large language models (LLMs) are often using full
                attention and a length limit (i.e., without any modification to their attentions).
                What about extending the context length limit of a trained LLM at inference?
                    Position Interpolation: estimate the position embedding out of the maximum length limit of tokens
                    KV Cache: cache the important key-value pairs from the attention in the cache memory, isntead of storing all kv tokens.

        Evaluation and LongContext Tasks:
            Simple: Perplexity, classification over long documents
            some previous works: 
                - Sentence scrambling (Barzilay and Lapata 2008)
                • Final sentence prediction (Mostafazadeh et al.2016)
                • Final word prediction (Paperno et al. 2016)
            Composite benchmark containing several task: Long range arena (Tay et al. 2020)
            Needle-in-a-Haystack: Multi-document QA task: retrieve the correct answer from an long text string and measure the accuracy.
                info hidden in long document, ask model to retrive some info that hidden info.
                Retrieving string in a JSON object. to retrieve key value mapping.
            LongBench: A bilingual, multi-task benchmark for evaluating LLMs in handling extended documents and complex information sequences.
                seq up to 40,000, multi doc QA as well. few shot, synthetic, summarization, code, single long doc.
            Entity Coreference: classifcal NLP for evaluating long context, find entity pronoun is referencing. 
                doc level tag which pronouns refer to whom.
                idea: Identify Noun Phrases mentioning an entity, then Cluster noun phrases (mentions) referring to the same underlying world entity. i think sequentially
            
            Discourse Parsing: find some discourse/logical correlation among sentences in long document. help train with relation trees among sentences in document

# LECTURE 14: KV Cache in LLMS

    KV cache for efficient inference of LLM
        Long context is needed - 4k to 1M , agents with long history and long doc understanding.

    KV cache - stored intermediate key and value computations for reuse during inference. adds complexity to code implementation, increase memory requirement(GPU), speeds up inference.
        example, augoregressive decoding has redundancy token generations passed to next to generate later token, for kv cache store prev decoding step key val instead of recomputing for next generation, result: each generation step computes one K/V(using cached K/V for prev steps) instead of its K/V and forward pass calculating for all prev token.
        sacrifices memory to gain speedup(reduce computations), cant do best of both.

        Transformer attention and KV cahe mechanics: at time t new tokens KV vectors are calc and appended to the cache. everytime decoding we reuse it instead of recomputing.
    
    LLM inference Bottleneck - to store vectors to allow largen context size instead of recomputing all prev keys. Reuses cachced per layer key/value tensors for past tokens. 
        Memory grows goes fast O(LxDmodelxNheadsxNlayers) limits. i.e. LLama 3B model 1M token, most of memory used by KV cache(120GB). but 64K(8gb) is half of weights(16gb)
        more than 100K tokens KV cache dominates GPU and makes single GPU inference impossible, need to distribute to other GPUs(transfer loading, network transfer)
        obj: compress or selectively prune KV cache with iminmal loss in model accuracy(called KV cache compression).
    
    KV Cache Compression 
        Prefill stage: process the input, produce and store KV tensors for all input tokens.
        Decoding stage: for each generated token, model attends to cached K/V.
        implication -> some methods operate at prefill(compress stored cache), some during decoding(online eviction/compression)
        
        Compression strategies-
            - token selection
            - quantization/low-bit/reduce token footprint
            - low-rank & franctorization(approx KV matrix)
            - encoding/ gzip-style tensor encoders for network transfer
            - Streaming & architectural approaches (pattens observed i.e. attension sinks, heads splitting, etc)
            - Redundancy - aware / semantic chunkking
        
        Eval metric: memory reduction, generation quality(perplexity, task accuracy, human eval), latency&throughput(prefill time, tokens/sec decode), implementation complexity and comabitlity(no finetune vs requires fine tune)
        
        Mehtods: 
            H20: Heavy hitter orachle - obsereve atention mass is highly skewed - small subset of tokens accrue most attention called heavy hitter.
                identifies these tokens and keep KV cache, discard or compresses others. Works as an online selection approach, compute token importance and main a compact cache memory.
                how to find heavy hitters: compute accumulative attention score for each KV vector, keep the top k scoring KV vectors.
                Can reduce 90% of KV cache memory while keeping ~78% accuracy compared to full KV cache.
                memory efficient, simple to interate into inference loops. Required scoring token-some overhead, can fail if nonheavy tokens are crucial for downstream reasoning.
            StreamingLLM: (2024) key empirical osb - model develop attention sinks - specific tokens typically earlier ones will attact a lot of attention. common among many model familities.
                in later layers model focuses all attention to early tokens, model just throws attention to earlier since it doesnt know.
            
                    sink reason unknown(if model doesnt know if relevant, chooses to give attention to earlier one.)

                combine a rolling sliding window of recent tokens with a tiny set of sink tokens kept indef in cache.
                
                generate token beyond token length pre-training token length comparision of methods: - dense attention perpelxity incerases drastically(Cant handle), sliding window attention 
                    calculation inaccuracy since cache window in first few sink layers - input length exceeds cache size., streamLLM keeps sink in KV cache as window moves. 

                enables coherent generation with bounded memory, no model fintuning, implementationn - keep global sink KV entried + windowed KV, small engineering changes to attention computation.
            
            SnapKV: llms knows what you are looking for before generation, key obs aech attention head consisntly focises on specific parts of prompts,. tHis pattern can be obtained from obs window at the end of prompt.

                Needle in a haystack - test to retrive imp info from long context used to measure performance. original mode can only support 33k tokens but with this can support almost 10x more with few accuracy drops.

            Pyramidal Info Funneling(2025): Uniform Distribution in Bottom Layer in earlier layer(linear line) but as layers go on we find localized attention(stairs), also see attention sink only happens on earlier layers.
                existing works in kv compression: dense full kv cache, streaming lLM,SnapKV and H2O (fixed kv cache per layer on attention score within insruction window).
                observation, higher layer with fewer KV cache budget and lower layers with more KV cache budget. each layer will have differnet budget, allocated adaptively across layers.
                i.e.instruction tokens retain the KV cache for the last few tokens , higher layer few tokens. lower layer more budget.
                has hyperparameters to find shape for budget distribution. compute important score for each token on attention, slect k token based on imp score to retain each layr used for allocation.

                compared to al lmethods - method outperform all others by significant margins as budget size increases(avg KV per layer).
                extreme memory efficient scario, PyramicKV outperforms baselines on some tasks by large margin.(incontext prediction as well)
                    in needle-in-a-haystack can match 99.6% performance of full KV if kv Size =64 can still match. inrease kv size 100%.
                    (KV Cache Factory - easy to use KV  cache compression) - github codebase to play around with optimization.

            R-KV (2025) redundancy - aware KV cache compression for REASONING MODEL
                handle overthinking ussue: existing KV cache compression leads to loss of info for reasoing steps. for agetic, reasoning, etc.
                causes increases rethinking step is KVcache strategeis like SnapKV. Idea: Eastimate the information absed on comparing all the key pairs, 
                    compute cosine similarity across differnet keys, use that to estimate an imprant score and redundancy score to selec keyval vector to store in memory.
                    helps with overthinkging

        Open research problems: 
            provable gurantees - hard to obtain reason, when can we safely remove token without loss of capability/info.
            more adaptive way for compress for training and finetuning.(curr only inference time - maybe if in training can be passed down)
            multimodal kv cachce and cross modal reundancy handling ((i.e. video, image -slight movements different))
            better benchmarks for loong reasoning under compressed KV.

# LECTURE 15: Retrival Augmented LMS: (PAST,Present,Future)
    Retrival Augmented LMS can reduce hallucinations in long-tail knowledge
        adaptations w/o training
        can swap data store(incrementally or fully) to keep up to date.
        more parameter efficient to language model(similar compute, rag performs better)

        before: 2017
            intitial stidied for NLP, mostly QA tasks.
                cannot use small dataset to learn all QA so RA was proposed.
            2019:
                ORQA, RALM RAG,...

            2020,2021 KNN LM, RETRO - new achitectures for RAG LMS.

            2020 chatgpt got access to search engine to grab most recent info, and got better than most QA model.

        current:
        2023 - incontext retrivalAugmented LMS - use fof the shelf lms and retrival models


        in the past, model based on where we incorporate retrieved ocntext. typically input side, feel to LM. Motivated several(i.e. REALM) classical models to retrieve index of relevant value/text band that answers question. not gnereation.
        meta proposed RAG - instead of masked LM to predict text span, we use autoregressive BART to retrieve answer in natural Language. qeustion encoded to vector, use MIPS(knn) to find k paragraphs, feed it to generator(LM) and marginalize probability y given x.
            RAG performs much better than REALM and other baselines on QAs.
        pros and cons of inpu aug:
            powerful to switch nto new LMS w/funetuning or without. can change dataset.
            expensive to scale up to hundreds or thousands of documents, no strict attributions to specific evidences.
                accuracy drops massively even though model can support large context.
        
        retro incorporating context in intermdiate layers.
            endables more efficient incorporations of many documnetations. 
            frozen bert model as retriever, and grab nearest neighbor. have query split, encoded layer by layer for each chunk, bert retrieved neighbors recriteved context from datastroe, cross attention(done with some ontext of past split+retrieved context, block(ATTn + RET + FF)) and integreate to next layer representation. 
            is very good at integrating many documents, perplexity goes down. even to 1.7 trillion data points. increase retrieve doc for computing matrixs up to 40 documents best score.
            Instruct Retro on top of Llama, good for pretraining, and multi task.
            pros: can efficiently incoporate many paragraphs into vector than input augmentation.
                possibly more effective than retrieval augmentation.
            Cons: required modified underlying LMS, expensive pretraining necessary, doesnt provide strict attribution.

        output interpolation: incoporate output with LMS, KKN, LMS
            given contex, a model predictss distribution of next token.
            knn lm computeds non parametric dist, by finding similar training context. using embedding match(instead of string match like Ngram)
                linear interponation with LM prob and KNN prob. inporating output porbability distributions.
            performs better than parametric LMS, and 30x larger 3B lms with larger datastore.
            tradeoff between efficiency and domain adpataion
            pros - provides token level attributtion, enables explicit control between parametric and non-param memorizes
            cons - hard to scale to large retrieval corpa - num of embeddings equal to num of tokens. empirically shows limited effictiveness outside of upstreams language modeling tasks(liek reasoning and text generation.)

        [check img]
        Present: RAG with LLMs
            simply augmenting input of LMs gives significant gain across tasks. augmenting input space of LMS give significant gain across differnt tasks. sig gains in downstream tasks like QA.
            BM25 works best supprisingly.
            Limitations:
                in-context retrival somtimes generate context that is not fully supported by their citations.
                They can easily be distracted by unhelpful context.
                diverse tasks require diffent retrieval needs, conten, freq, may require iterative rertrival based on curr generations, questions with similar solutions may have limited semantic similarities in embedding space.
            considerations to improve:
                optimize 
                    LLMS: SAIL - train model to identify which context is useful factually for answer - sythenicly generations solns with NLI model, 
                        SELF RAG - teaches lms to adaptively retrieve and evaluate context and own knowledge for generation, another critic model to see retrieved context tags is relevant or not. if relevant tag of relevant, checks if ouputs of model and dataset are supported or not at test time.
                        new training better.
                    
                    retriever: 
                        train retriever and freeze LM. useful when only have acces to api. REPLUG: train retrievers for black box llms by minimizing kl divergence between lm nadretreiver, if corrent next token then know which doc useful. helps retreiever selects most relevannt doc.
                        RADIT - trains both retrieever and ML on multi task objective. 
                        better perm for combinations of off the sheflf.
                    prompts: 
                        training free rag system, not robust to differnet promp templates.
                        better to structure prompts is the idea.
                        DSPy optimizes instructions and few shot demonstrations to acheve the best performance. bayesian optmization to optimize prompt.
                        

        future: 
            infra: challenges of scaling up datastores and increased inference time costs. better perm by increasing datastores but increases inference costs, memory, storage, etc.
            algos: has efficiency redundancy
            eval: rag can be useful for domain specific. going beyond domain dataset(only eveled on QA or multi choice QA, more comprehensive benchmarks), instead of merely evaling final correctnes,more holistic eval i.e. quantify citaion correction, link valid, generated supported by content or not?

# LECTURE 16: Mixture of Experts in LLMs
    for scaling models
    
    prob training and inference cost grows as model scales.
    key idea; not all params need to be active per input, dynamically route tokens to specialized sub-networks.
    what is MoE - replace big FF with many big FF networks and a selceor layer, you can increase num of experts without affecitng FLOP.
        neg log perplexity decreases as count of models increases. faster training as well x7, better training loss ~3x less FLOPs.

        parallenize across many connected gpus to run faster?
        MOEs are most of the highest perfm open models, and quick.

    why not more popular, instra is compelx, training objs are heuristics and sometimes unstable.

    implementation: add multiple ff/Moe Layer instead of  mlp laeyr is typical. Moe for attention heads are less common and not seen as effective.

    Design Varies - routing fucntions, expert sizes, training objs.
        routing: most boil down to tok k, either tok k of each token(expert chooses token), or tok k for experts(choose expert and optimize that - but sometimes might not be best), Global routing via optimization - very costly - not popular.
            learned param(linear layer/router scores) to run topk used in most MoEs, 
            another is just hashing used as common baseline.
            RL to learn routes - used in some earliest work, ubt not common too expensive to train routers as many layers many rouuters, impractical
            Solve a matching problem - linear assignment to resource allocate, for routing, complextity for learning selection network, also not used today.
        
            TopK most old and classic routing used, colms in matrix are experts, run softman then topk, then weighted FFN of top models..

        variation, need to determine small shared(1-2) experts, and large expert FFN networks, and activation you can activate 2.
        Ablations of MoE - increasing experts higher performance, shared experts doesnt see a clear improvement.

        How to train MoE - we need sparsity for training-time efficiency…But sparse gating decisions are not differentiable
            reinforcement to optimize gating policies
                high ariance, need to add training complexity on top of MoE - hard to implementalgo, does work but not much better that its a clear win(even over hashing based), not widely used.
            stochastic perturbations - for exploration and exploitation
                routing decisions are stochastic with gaussian perturbations, experts a bit more robust, softamx allows model to learn ot rank k experts, adding noise a way to allow stochastic randomless: min amt of noise does not route to best, a lot of noise unstable. explored for a while but then abandoned.
            heuristic balancing loss to encourage learning for router.(typically used, can work to stabalize the unstable nature)
                systems efficiency requires that we use experts evenly, examples: per expert balancing same as swittch transformer, Per device balancing aggregated by device.
                every expert needs to recieve appropriate num of tokens to calculate prob. 
                if less token more weight bias, if more tokens certain FFN gets lower bias. to accumulate bias over iterations to balance workload over experts. Extra bias bias for each expert willg et tokens in online setting. Aggressive balancing 
                per-expert - balance workload across mini batches per experts.
                per device - seq wise balancing, for each seq blance tokens across different experts, i.e. each token passed to experts blananced, more aggresive version, to balance workload during training.
                overview: if we remove heuristic blancin lloss we see model will use 2 of them, with balancing all experts get same amt of workload. all experts utilize compute properly for training.
            traiining systems side - nice way to parallelize computation across differnet devices. good for larger scale training.
    
            Tricks to stabalize  - high precision for router network to make it accurate, else router wont be accurate.
                    fine-tuning - can overfit small fine tuning data,  some sparse moe module will overfit compared to some dense model, overfitting issue, to solve use large amt of data to avoid overfitting. 1.4M supervised data used by DeepSeek to tackle this.
                    upcycling - trainign pre-trained dense to  moe models for initializing, shows gains from base math models.
            
        Deep Seek Moe Architecutre 
            V1 - standard top k routing(discussed before), standard aux loss balancing(expert+device) trained with 2 shared expert and 64 fine grained experts
            V2 - same routing but larger params and more active params, balancing loss balacncing both comms in/out system level(among devices). topM device routing.
            V3 - more params, more experts, have shared ffm, normalize score after topk, renorm after topk before doing weighted. aux-loss free+seq wise aux(bias updates in online setting, add/sub gamma at each iter to record what prev workload was in each workload)

    Summ: takes adv of sparticy - not all inputs need the full model.
        discrete routing is hard, but top-k heuristic version is simple enough and works.
        lots of emphirical evidence now dayys show that MoEs work and cost effective to train for fixed FLOPs.
        extra: n num of experts -trade-off between model capacity, training cost, inference speed, and system architecture. 
            thresholds - score all experts and take top k, no thresholds.

# LECTURE 17: Machine translation and multilingual NLP
    machine translation systems - find corresponding words and meanings across languages. parallel corpus - i.e. menus english and chinese version, books, etc.

    previous efforst for translatio w/ noisy channel framework.

        model two translation conditional probability.
        noise channel sample englih pass through channel to foreign, and ask listener to translate to english.
        argmax to find most likely enligh, bayes rule to decode into p(e) xp(f|e), useful can leverage normal data + extensive english data now(LM).
            estimate pron with MLE of counts of words. w/ parallel corpus.
            find word alignment mapping between two languages, w/ dropping, insertion, one-to-many, many to one as well.
                hey issue: alignment prob depend on word prob.

            use EM algorithm, pick random starting param,  compute expected alighment, update no of e is translated, could e,f, and keep track of no of times e is used in training corpus count. use mle to update translation probability.
            idea use iterative estimation for learning learning probability.
                later used for phrase level.
        
        Neural Features for Translation, n-gram LM, predict next given prev n works and alighed source word and its m neighbors.
        fully neural tranlation - no need allighhnment, compress all info to last layer and pass to decoder.
            Attention - struggles with long sequences. lot of information loss, so look at small segments of relevant words.
        Transformer - instead of using RNN to encode, use self-attention. can scale up model size, and make connections, higher performance.
        Googles MultiLingual NMT, multi layer encoder and decoder, allows different languages to different languages, based on indicated language. interpolates language code embeddings.
            showed similar language structures in different languages of similar sentenses.
            bilingar pairs vs multilinguar - high resources, bilinguar models better, but for when low resources multilingual model better due to some transfer.

        Xtreme: massive zero shot cross lingual transfer learning - benmarch how transferable are representations
        Hybrid Mt: KNn prob and language probability used by machine tranlation -> do linear interpolation from that. 
            prompt based mt - given translation instruct and few shot examples ask llm ot perform mt. useful for domain specific translation. will have some accuracy for terminologies, consistency. 
                can incldue translation table as part of prompt, use corresponding tranlsation if possible to maintain correctness, while leveraging LLM translation. wokrs fairly well. can ask to leverage to optimize-more prompt creation.
            data contamination allowed gpt 3 to learn language translation implicitly.
            llm mt worked great for high resurce language - comparable, low resoruce NMT (Google Translate >= NLLB >> LLM-MT)models better.

    Summ;
    Comparison between SMT and NMT
        • SMT (non-parametric model)
            - Explicitly store translation phrase pairs
            - High precision, but low coverage, thus poor generation
            - Easy manipulation (insert/delete/update pairs in the translation phrase table)
        • NMT (over-parametric model)
            - Implicitly store translation phrase pairs in model parameters
            - Better generation, but data-hungry
            - Hard manipulation (catastrophic forgetting after fine-tuning, domain adaptation  issues)
        • Hybrid MT (e.g., Retrieval-MT, Prompt-based MT)
            - Leverage both neural network parameters and retrieved translation phrase pairs
            - Perform better than NMT on domain adaptation when providing new domain information from retrieval.
    
    Future - tranaltion not just languages, but thoughts, contents, cultures, etc.
        challenges: data scarcity, evaluative metrics.
        some work: eval cul awarements of MT method, and compate NMT vs LLM MT.


Course: https://junjiehu.github.io/cs769-fall25/lectures/ 