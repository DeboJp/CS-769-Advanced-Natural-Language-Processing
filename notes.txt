# LECTURE 2
    Lexical Semantics: Meaning of word
        lemma(title), sense(different meaning within diff context), definition
        connections between words (synonyms, opposites, etc)
        taxonomy - absract(general words) to concrete(specific).
        Semantic frames and roles -> to denote perspective or participants in an event.
        
    Problems with discrete repr:
        one hot vector(give each word separete numbers on vectors for 1): sparce, subj, expensive, hard to guess relationships.
        goal: to map similar meaning workds closely, can use KNN to find close words. Standard way to repr mean ing in NLP.

    Training of word embeddings:
        Wrod2vec: 
                Cbow(opp of skip gram) - from many guess one.
                Skip Gram: use imput to geuss bunch of words (define a sliding window and train a classifier with padding, 
                            to predict correct token for all sectences. roughly self supervised.)
                            - some words used a lot, learn a lot of repr in language.
                            - others used less, more concrete, meaning very unique.
                            uses ONE HOT + embeddings to train/predict surrounding/similar word repr.
                            - loss function log probablity(dot product to predict similarity between surrounding words and 
                                    normalize with all words -> expensive so approx by negative samplinig(sampling without main 
                                    sliding window but vocab is not all unqiue - but can use freq for norm) idea grab random samples 
                                    to do sum of pairwise normalizaiton -> convert to binary classification-easier to optimize and 
                                    learn repr) then use some gradient decent to predict.
                            - evaluation: check quality: instrinsic eval- test repr align with intuition about word meaning, using cosine similarity of word embeddings.
                                        extrinsic eval: test weather repr good for downstream tasks, such as taggin, parsing, QA or some task used by some model.
    Text Classificaiton: generative vs discriminative.
            generative - prob joined with and ouput, argmax over joint prob.
                - prob of input first, probably more expensive.
            discriminative - prob output given the inout data, argmax over conditional prob.
                = directly models prob with output space.
                - BOW(bag of words) discriminative model, does not care about word order. get score to prob then gradient descent. but for neural networks can use sequential order.
                
            logistic regression good for binary classification. sigmoid for binary, softmax for muti-class to repr to valid probabilty.
            gradient descent - gradient of loss function and use chain rule. to move to direction that decreases the loss. lr is the speed of movement.

    Q: norm samlnig denom is some sort of avg to shorten error?
    Q: so do we use freq when norming or no? since Vocab is not unique?
    Q: how does input become smaller dim(PCA?)?

    HW1 classification example:
        Continous bag of words(CBOW) has lower dimension, just bag of words(BOW) is one hot size of each lookup is number 
        of vocab. + linear layers. then softmax function and optimize with gradient descent and regulizer.
# LECTURE 3 : Language Modeling
    Ngram Language model, Neural Model, Evaluation

    define: final end of sentence token is special. 
        problem is probability of grabbing sentence from 

    What can it do: score sentences(high/low), or generate sentences(prob: of next work/token)

    N-gram Language model:
        every word is independence, and can use maximum likelihood MLE to see how likely eaach word is supposed to appear(by counting).
        - allows prediction of whole sentence too.
        - want to score with unknown read words,  interpolations w/ UNK model enables to allow low frequency unknown words.

    higher order model: limit context length to smaller window(n-1). MLE is count and divide. linear interpolation to deal with zero counts.
        sometimes add one smoothing, every word adds 1 count.

        other smoothing techniques: additive, discounting, kneser-ney. most of them to address zero count issue.
    
    Limitations of Ngram:
        Ngram cannot share strength among similar words, need class based language models.
        cannot condition on context with intervening words. need skipgram language model.
        cannot handle long-distance dependencies, need cache,trigger, topic, syntatic moels, etc.

    Linear models:
        cant learn feature combinations, logical errors - cant be expressed by linear features, use neural language models.

    Neural ngram model(since proximating current based on previous, but using neural model to learn embeddings):
        convert word predition to discriminative text classification.

        feed-forwrad neural language models. - uses previous words, looks up word embeddings, combine linear vector to get intermediate 
        vector + bias to get score and then get probability to get next token.
            (grab two d dim vectors and grad a d vector absed on that, convert to v(vocab) dim, softmax to prob)
        
    
        Solves: simililar word/context issues, intervening words, cannot handle long-distance dependencies.

    neural networks allow design of complex functions.

    PROS ngram: models are extremely fast, better at modeling low frequency - TOOL: kenlm

    ngram lm(non parametric-non training just statistics), prefix context table and counts will be massive to store. to compute just retrieve row.

    neural counts but also compresses, memorizes and generizes.

    Non-parametric: • Memorize low-frequency pairs of (prefix, next word) • Cannot generalize to unseen pairs • Interpretable & easy for incremental updates (insert/delete/replace)
    Parametric: • Learning from data-driventraining • Good generalization tounseen pairs • Black-box & expensive forparameter updates withforgetting issues

    Why llms so powerful: 
        - Architecutre, MLP then RNN and now transformers to capture sectquecnes and effecienceis to scale to 100B+.
        - Pre-training on massive raw texts.
        - Fine-tuning on language intrustiosn with supervised learning or RLWF.

    LM Evaluation:
        - log prob of sentence over test corpus, aggregate them log likelihood score(not comparable to different test sets), so we normalize it by count, turn into per word cross entropy and grap perplexity.

        unknown words: 90% of words are low freq, cut size of corpus(by unk or rank threshold), treat 90% as unknown words, to save compute.
            - subword tokens can handle loss of accuracy by using sub tokens instead of throwing tokens away, also handles mispellings.
        
        IMP: cannot compare models over different vocab. 
            Or more accurately, all models must be able to generate the test set (it’s OK if they can generate more than the test set, but not less)
            • e.g. Comparing a character-based model to a word-based model is fair, but not vice-versa

# LECTURE 4: Recurrent Neural Network 

    General knowledge:
    - when working with minibatches. need to add artificial token to make sure each sentence has same length. add paddings. (PER BATCH ONLY)
        - when computing avg we should zero out padding tokens, sum out embeddings over word and divide by number of real tokens-non paded length.

    Sequential:
        long distance dependencies in language can be compicated, nouns, or references such as 'it'.
    
    Recurrent Neural Networks:
        Approaches:
        1. variable size inputs: add zero paddings and concatate + Linear layer -> simple but downt works for longer sequences. massive ff layer.
        
        2. feed one word to each feed forwawrd layer : for each token dot product with weight + bais, and pass to next token. 
                (idea compress and pass info to next token to carry on, avoids super long W matrix.)
                do not need to pad senteces, and each ff layer is smaller than previous method.
                maiin idea for RNN, we can have many layers as we want, variable depth network. remembers information along time dimention.
                we can predict per token vs label, grab loss and sum them up for total used in backprop. backprop updated weights(rnn) not inital hidden layer passed on.
                backprop over time sequence - issue vanishing/exloding gradients. if we do back prop since al weights are shared.
                    - handling vanishing gradient: LSTM - long short term memory. make additive connections between time steps. gate to contril info flow, and 2 mem cell to store short/long term info.
                        - forget gate-how much forget long time mem, input gate-how much to add to long term mem, compute new values to add to memory, output gate-how much to pass on from long term to short(next step).
                        - all is passed to next time step(short term - only some portion of long term-higher freq of updating, allows prediction per token in seq).
                    - handling exploding gradient: gradient clipping(general idea can be applied to others). we norm by some number before updating, basically making it take shorter steps, instead of massive ones in gradient descent.

                exposure bias/distributional shift: training we pass in ground truth per token, but during test we pass on predicted words to predent/finish sentence. since its a prob dist model might sample an unknown sequence.
                    - happens for generative tasks, not classification.
                    - to handle:some reinforcement learning or scheduled sampling. first train on ground truth for some iter but gradually switch to taking its own input, and always continue like that. 
                                        - cannot start with pass in its own input, or else it will learn garbage. start with supervised then reinforcement function.
                                        - handles model robustness to handle new sequences, not necessarily accuracy. 

                                        Q: do we have to write our own feedback func to say weather new seqs in training during exploration are good or not?
                                            - model is baked in to explore some similar but wrong words but should later still learn the proper sequence. so overall message is not lost. not necessarily RL but SL + noise.
                
                Applications:  represent a whole sentence to make pred, represent a context within a sentence(read context up intil that point).
                                - can do 1-1, 1-many, many-1, many-many, many to many fully conencted.
                                    image captioning, sentence classification, machine translation, sentance tagging.
                                
                                - can do binary or multi class prediction.sentence repr for retrival, sentence comparison, etc.
                                - sequence labeling, calculating reprs for parsing.
                                - language modeling - tagging task, each tag is next word. is a auto regressive model(curr depends on prev token.)

                                bidirectional rnns - to handle two directional languages, also llms use it, also useful for sentence repr.


                What can LSTMS learn: 
                ...

                Efficiency tricks: minibatching makes thigns faster. harder in rnns than feed forward networks. since each word depends on prev word. (better to separe padding and end of sentence token)
                                - using a masking token to zero out paddings. to exclude it when calc loss. and divide sum of loss by real tokens in minibatch.
                            
                            bucketing/sorting - put sentences of same length with same minibatch. but can result in decreased performance.
                
                handling longer sequences: cant fit massive lang in mem, truncated segments. separete it into 2 passes but connect end of 1st to start of 2nd's hidden layer.
                                            - backprop individually though.
                
                Gated Recurrent Units: LSTMS but way more efficient. with better parameters.
                architecture search - basic lstms pretty good(the various combinatoins gates searched.)

                Multi-Scale Pyramid RNNS: one for word, one on top for sentences, one for paragraphs.

# LECTURE 5: Attention and transformers
    Encoder - Decoder models: used to be popular, nowadays only decoder(encoder merged within). prob, compressing sentences of varying length to single vector, hard to decode later on.
        - idea use mulitple vectors bsed on length of sentence: attention.
    Attention: embed vocab to dict, query a target element, pick relevant sources by comparing query/key-value based on confidenc. summarize relevant work tokens into a contetxt vector, used to predict target token.
        encoder gives keys and vals to each word based on learned functions, decoder creates queries to check/find relevant key. we dont use argmax on keys (not differentiable(is discrete)) instead use softmax (to smooth attention dist), is differentiable, allow backprop.
        after grabbing query results, we grab norm attention weights, create a context vector with weights + base word vectors, and pass it though a FF network + predicting base word vectors to predict per label. 
            Feed Forward:
            - multi layer perceptron great for large data (applied position-wise after attention)

            Score (attention alignment):
            - dot product; if too large dim, use scaled dot product
            - bilinear (general form): score(q,k) = qᵀ W k
            - MLP/additive (Bahdanau): score(q,k) = vᵀ tanh(W_q q + W_k k)
    
    transformers: adv: really fast, only matix, parallel sequetial processning, efficeint.
        - self-attention - allwos parallel comp of all tokens.
            attends to itself most, but also givs attention to other words/elements in sentence, context sensitive.
            using embeddings to grab key,query,value. use querry to grab ino from each other embedding, use softmax on all. use values and softmax dist to grab sum. saically summarizes its own info and other words info. can stack multiple self-attention layer.
        - multi headed attentions - allows querying multiple positions at each layer.
            multiple attention heads focus on different parts of sentence(can encode in parallel).used several heads per set, t heads to create a single d(t) dim vector.
            add nonlinearity - FF(MLP), so output is no longer liner to input, add capacity to model to make it complex. we stack self-attention(multi head) and non-linear layer N times to increase capacity.
        - position encoding - addds position ino for each token.
            model perspective no position info, since parallel. add soe info function at the very beginning to augment input where word is location.
            naive = integer index and another dim to input. issues embedding vector also numeric, so may cause instability.
            we use frequency based functions, sigmoids with diff frequency. differnet dimentions say differnet info. 1st even odd indicator, layer first-half etc. each word have added dims. combination if some of those dims give positional info. typically concatate with word embedding info and pass them as same.
            another idea add learnable parameter for every input. but fixed size for length, need to estimate or model might not work. typically more optimal but more complex.
            typicall absolute not useful, relative to other words more useful.
        - adding non-linearrities - combiens features from self-attention layers.(expl above)
        - masked decoding - prevents attention lookups in the future tokens, to avoid predicting future tokens.
            avoids looking into future tokens, masks attention to future tokens(keys).(step before making softmax dist). future token will not affect context.

        self attnetion: combine words
        multi-head = learn independ head information
        norm dot prod atten - to remove baias in dot prod calc
        prod encoding - without RNN, to udnerstand location.

        Training tricks: layer norm - avoid number blowouts, training schedule - adjust from adam (study show needs some warmup from small to large then exp decay to find optimal lr), llabel smoothing - insert some uncertainty to training proc(allows sampling since finding a single point is rly hard).

        Attention hard - slow at test, doesnt necessarily outputform RNNs on decoder side for seq2seq taks (transformer need a lot more data), hard to train on small data.

        EXTRAS:
        Additions: can add markov properties i.e. add attention matrix. encourage sparcity: instead of softmax to smooth out, ask model to pick a few tokens instead each time.
            incorporating markov properties: attention from last time correlated with attention from this time. add attention from last for next deision.

            Hard attention: instead of softmax, use zero-one decision. harder to train, needs Reinforcement learning, perhaps helps interpretability.

        Better Training: 
            neural models tend to drop or repeat content, add penanty if attention not 1 over each word.

            attention is not alighment, often blurred, off by one on matrix for translation, and other tasks too.

            supervised - pretrained + supervised is strong alignment model. but tons of data, model can generally learn too.

        What else:
            Copy Mechanism: model can not only generate from vocab, but also copy tokens directly from input/history, final prob = mix of generate prob + copy prob

            hierarchical attentions: attention over words then attention over sentences then document. grab context of document.

            attention allows various modalities, vision, text, sound/freq, etc.

            multi source attention.

            basic idea of attention: idea similar to info retrival.

# LECTURE 6: Pre-traind Sentence and Contextualized Word Reprs.

    why pre-training, 

    non-Contextualized - word2vev, seq ,etc.
    contextualized - bert etc.

    pass non-context though an multi layer perceptron to turn into contexualized?

    why pre-trainng: learn multi-tatsk learning, transfer learning - finetuning

    NLP TASKS: text - language modeling, Natural occuring data - machine translation, hand-labeled data - most analysis tasks.
        - 1. Perform multi-tasking when one of your two tasks has greatly fewer data, 2. Perform multi-tasking when your tasks are related

    multi-task leaning, train reprs to do well of multiple tasks at once, i.e. tagging, language modeling.
        - should be table to train encoder on large dataset for one obj, then use same encoder+finetuning later for other tasks such as tagging.

    3 main objtives - define model, define training obj, data.

    end-to-end - is done if easy or has enough data points. else always use pre-training.

    Usage of sentence reprs: 
        Sentence Classifcation - sentiiment, classify, binary labels over sentence, etc.
        Paraphrase identification/Semantic Similarity - compare meaning semantically between phrases
        Textual Entailment - learn implications(Entailment), Contraditions, Neutral.

    Pretraining Techniques:
        (idea model pretrained for cetain task, then encoder/embeddings used from that to be used in another task)
        eg 1: Google: model: LSTM, obj: LM obj, data: Classification data itself or amazon reviews. downsteam on text classficaion, init weights and continue training.
        eg2: GPT: Masked self-attention(transformer), LM obj, Data: BooksCorpus. downstream some task finetuning other tasks additioanl multi-sentence training.
        eg3: Auto encoder + Transfer: LSTM, from single sentenve vec re-construct the sentence. Data: clasification data itself or amazon reviews. downstream on text classification init weights continue training.
        eg4: Skip-thought vectors(sentence level contet): lstm, predict surrounding sentences, data books, imp because of context. downstream sentence pair classification. 
                (Similar to Skip-gram that predict the surrounding words by the center words)
        eg5: paraphrase based contrastive learning: try several model, predict weather two phrase are paraphrases or not, data: paraphrase db from bilingual data.downstream: sentence similariy, classification, 
                (LSTMs work well on indomain but word averaging generalizes better.)

        deep-fusion - concat sentence for grabing self-attention(sentiment - close, deep,repr, efficient)
        late-fution - do encoding on ecah then gram.(sentence classification - sparse, more time?)

        eg6: Entailment + Transer:Bi-LSTM + max pooling, (use human labels for pretraining) supervised training for a task such as entailment learn generalizable embeddings. Data: Stanford NLI, MultiNLI, 
                (Tends to be better than unsupervised objectives such as SkipThought)

    Context word repr:
        Central Word Prediction:
            context2vec: Bi-directional LSTM , Predict the word given context, data: 2B word ukWaC corpus. downsteam  use vectors for sentence completion, word sense disambiguation, etc
            ELMo: Model: Multi-layer bi-directional LSTM • Objective: Predict the next word left- >right, next word right->left independently 
            Data: 1B word benchmark LM dataset. Downstream: Finetune the weights of the linear combination of layers on the downstream task

    Masked Word Prediction:
        Masked Word Prediction (BERT):model: Multi-layer self-attention. Input sentence or pair, w/ [CLS] token, subword representation, obj: Masked word prediction(mask certain words ask predict) + nextsentence prediction((are they close or far))
            Data: BooksCorpus + English Wikipedia (16GB)
        
            select 15% of words randoly, replace with mask 80%. time, 10% replace random(to simpulate noise), 10 no change. predict all maskd words given surrounding context. like context2vec but better suited for multi-layer self attention.

        Consecutivee Sentence Prediction: classify two sentence as concsec or not. data = 50% of trainngin from openbooks is consecutive, use unique cls token to include meaingn of two sentence. mask some sentece in sentence and sep tokens to separaee sentences.  and binary predictions.

        Hyperparameter Optimization/Data(RoBERTa) - same as BERT, but trainlonger and drop sentence prediction objective. but with much larger data. empirically much better.
            - sentence prediction is easy(doesnt help) if mask token prediction is done well(hard-loss goes down slow).

        Distribution Discrimination -ELECTRA: same as bert,Sample words from language model, try to discriminate which words are sampled. discriminator used as word encoding model later on.
            (masked sentence passed in generator, which predicts/sample words, discriminator then predicts if original word passed or not.) 
            (better than bert, more gradient updates to model since per token labeled, bert only does 50% as their tasks masks 50% only)

        Permutation-based Auto-regressive Model+ Long Context(XL-Net):Same as BERT, but include longer context, Predict words in order(itself and before), but different order every time(train sentences with different permutation orders: some noise but able to learn differnt perspectives). Data: 39B tokens from Books, Wikipedia and Web 
            - later version, instead of shuffing randomly, it group phrases to avoid random places, avoid learning wrong word combinations in sentence.
        
    Compact Pre-trained Models: 
        ALBERT (Lan et al. 2019): Smaller embeddings,and parameter sharing across all layers, but the same inference time as the BERT counterpart 
        DistilBERT (Sanh et al. 2019): Train a model to match the distribution of regular BERT
   
    SUMMARIES:
        Which model:
            - Averaging word embeddings can outperform more complex models, especially on out-of-domain data.
            - BERT shows the strength of bidirectional transformers but does not benchmark against LSTM models like ELMo due to lSTMs being constrained by computational efficiency and scalability.
            - With BERT-like training data, their ablations show architecture and training tweaks can yield further improvements.

        Which Training Objective? chatgpt
            - When using the same data, bidirectional language modeling outperforms a machine translation encoder.
            - Next-sentence prediction, combined with masked LM, seemed helpful in the original BERT paper(boosting perf), but later work (e.g., RoBERTa) found it unnecessary and even limiting when scaling training.
            - Next-sentence prediction provides little to no benefit and can be dropped.

        Which Data?
            - Preliminary results suggest more data generally improves performance.
            - Scaling with large web data yields gains but inconsistently.
                Data with context is probably essential/best.

        Pre-trained Large Language Models: 
            GPT-3: similar to GPT-2, but pre-trained on lot more data using autoregressive LM objective. Demonstrate good few-shot in-context prediction ability.
            ChatGPT: init form GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF).

# LECTURE 7:Parameter-efficient Fine-tuning - and other ways to bypass costly fine-training.
    PEFT - Fine tune a small amount of mdoel params, instead of entire model. on a small dataset of downstream tasks. Other parameters are frozen.
        pros - reduce compute and sotrage costs, 
            mitigate catasprohic forgetting
            easy to update model to new data and facts
            better perfm on low data
            comparable perom to finetuning
    
                                PEFT                    | Full Fine-tuning
    Learnable parameters:         A small subset        | Entire model
    Training Performance:         Close to fine-tuning  | …is fine-tuning :P
    Training Data:         Small                        | Large
    Training Time:         Faster                       | Longer training time
    Overfitting / forgetting: Less prone to overfitting | More prone to overfitting

    3 ways to adapt model capacity(Three Computation Functions): 
        function composition - add extra functions (combime multile as well) on top on NLP layer, (new function layer)
            - i.e. Adapters - additional modeluse in layer - match or outperfms fine tunign.
        input comp - combine data as input to create longer sequence. has effect on self-attention layer. (adapt data)
            - i.e. prompt tuning, prefix tuning (beginning of sentence instead of end)- context window of model is increases - good for large models
        parameter comp - low rank extra params in middle of the model, in matrix math area to increase wight by adapter wight. (adapt model with addl func)
            - i.e. Lora/QLora - no increase in model size, good.

    Function Composition:
        Adapter - adapter layer after each FF layer (before layer norm if there). Adapter itself is an multiplayer -2 layer perception.
            - each layer is a matrix which transforms the input in new space. adjust embedding direction. helsp reroute the data embeddings to what the "upper layer expects."
            - increase capacity of model to learn new date.
            - vs. full finetunign. small data better prfm with adapter, and about equal perfm with large data. better accury with fewer num of params updated. embeddings repr shifts little(origial meaning holds?) vs finetuning.
        
        IA^3: Add learnable params to self-attention layer instead on NLP, - model selects params that are moe and less imp for given downstream task.
            - adds learnable layer to MLP layer too.

    Input Composition:
        Prompting with text: prepending instructive words before actula test input. 
            - text tokens as learnabale params.
            - trigger mdoeling to learn tasks- 
            - models are sensitive to the formulation of the prompt and to the order of examples
            - Prompt Tuning: 
            Prompt tuning: only updates a small task-specific prompt parameters(learned, i.e. extra params added to left of embedding) for each task, enables mixed task inference.
                - during testing, add learned prompt to input.
                - as model size increases the perf of prompt tunign matching full fine-tuning.
                    - naivly few shotting model is still wrose than both.
                - only updates prompts, not x inputs concated.
            Prefix Tuning - more effective to capture gradient updates over layers.
                - adds larnable params at the beggingin on the input sequence of all layers. (multi layer )
                - better in low data regiuemes, better in general too - full finetunign.
                - only prefix updated, rest model frozen.
    
        Q: inputs trained to or just added to front.
            - no inputs trained, empty/learnable parameters added to front after capturing sentence embedding
        Q; prompts multiple voab or just 1
            - 1 or multiple your choice? More training?
        Q: Cost of adding adapters and input prompting compute costs?
            - there is added costs/params to both methods, but negligible.
        Q: Combination of these better in some instances or typically by itself better.
            - not necessarily, main idea is to find the sensitive spots and adjust them. There is a paper that is trying to find the spots and test multiple methods/combination to see if any better. Auto smth?
        Q: does fine tuning increase inferece speeds compared to original foundation model?
            - not inference speed, just training speed.
        Q: Does lora have any impact on forgetting? since we are adding to params? 
            - if matrix conflicts with other for the updates, depends on tasks if the knowledge confict w/ pre-tain knowledge. All methods can have forgetting no gurantee/non-gurantee.
        Q: Qlora explanation - quantization has some lsoss, previous does it affect performance? what dooes the bits reprepsent.
            - bits represent weights, we shink the specificity of the weights to a smaller "scientific number", not massive loss in performance if quantization done properly.
    Parameter Composition:
        Low Rank Adaptation (LORA)- approx self attenupdat of lernable weight by a low -rank matrix. initi update is 0, after training updates are added 
                back to original checkpoint. so at inference cost of the updated checkpoint is samea s orig checkpoint.
            -   save a lot of params, no actual overhead in inference. small overhead to do computation during inference.

        LoRa works better than other PEFTs (adapters, full finetunign, - prelayer, etc).
        why works: full tuning update typically happen in some low rank space, so we can try to update with low ranks as well.
        pretraining provides strong initization in D dimm space, model only needs to explore subspace during fine-tuning to learn final weights.
        Reduces VRAm consuption, fewer gpus and fewer I/O operations.
        lora still reqs forward comp and back prop. so lora speeds up 25% but noto 10x compared to fine-tuning
        
        QLORA - Convert information in a high-precision data type to a low-precision data type, Allows training a LLM in a single consumer GPU 33B in a single 24GB gpu.
            - lower precision avg. nowdadays. use fload 8 or mixed precision. - typically dont need very accurate steam for numbers.
            takes 32 bits to some rounded 8 bit representations, then after gradient updating on smaller bit dequant it back to fp32 representations.
            Double Quant: quantize the fp32 constant inside the fp32 bit, to further save memory on top of quantizing weight matrix blocks. dequant will have an extra nested step as well.
            quantization hsa some information loss, so we do blockwise quantization after flattening matrix(to avoid the max working on a large matrix), to minimize loss.
                - more sensitive to Hyperparameter, perm not same as full-finetunign due to agressive compression.
            can match certain fine-tuning methods with good methods.

        Check picture grpah to get and idea of eah model acc vs num of param.
            - depends on usecase which method better than others.

        Other ways to bypass expensive fine-tuning:
            - PEFT is one way to adapt model to downsteam taks
            - prompting incontext learning - doesnt require furthier training.
            - Retrieval Augment Generation(RAG) to steer model perfm - need to update model to use actual data.
            - Model editing - tuning free approach to modify behavior
                - computationally innexpensive, targetted modification to params. - if we understand causal mechanisms on internal layer
                - cons - kinda like blackbox. hard to understand?
                - Editing knowledge editing - knowledge store or update weights(locate and edit, constrained tuning)
                    behavior - inference time , activation editing, decoding time.
            
        Training models are sene as vectors: has additive propertives, skills embeeded in linear subspace, can combine to explore anothter trajectoty.
            - can take negation of task to unlearn, 
                - Can reinforce certan skills, or reduce certain skills. with combinatiosns (+ then (-), etc.)
        
        can steer model on self atten layer, modify attention heads. need good understandign of heads to steer or remove heads.
            - (incase its persay learning harmful info/hateful speech) can do targetted ediitng as well.
            - targetting heading better than fine tuning and other approaches as well.

# LECTURE 8: Instruction Tuning and Multitask Learning
    several tasks in nlp, only text-for language modeling.Naturally occurring data: e.g. machine translation. Hand-labeled data: e.g. most analysis tasks, 
    multitask: we can train models on many data to do well on several tasks.
    pretrain finetune: we can pretrain on one task and then finetune for later.
    prompting: Train on LM task, make predictions in textualized tasks.
    instruction tuning: Pre-train, then fine-tune on many different tasks, with an instruction specifying the task.
    
    Basic Fine Tuning: Build a model that is good at performing a single task
    Instruction Tuning: Build a generalist model that is good at many tasks

    Examples to build/evaluate machine understanding instead of just memorization.
        give context free question answering dataset/open-book QA, answer a prediction wihtout any specific groundnig knowlegde.
        give contextual question answering/machine reading/closed booking qa: Answer a question about a document or document collection
        Code generations: generate code from natural language command.
        Summarization: single doc-compress longer doc to shorter, multidoc-compress multiple to one.
        Info extraction: entity recog - identify which words are entities. Entity Linking - connect entity mentioned to another knowledge bases. 
            Entity coRef - find which entity in input correcpond to each other(him mean obama, etc.). Event Regoc/linking/coRed - identify what events occurred.
        Translation: from one lanuage to another. quality assessment done using similarity to ref translation.
        General Purpose Benchmarks - collection of general NLP tasks of past decade used to benchmark. to guage ability across broad range of tasks.

    MultiTask Learning: Perform multi-tasking when one of your two tasks has fewer data : High-resourced language → low-resourced language, General domain → specific domain.

    Domain - one task but data might be from several areas. sometimes labeled other times now.
        in practice: Content, what is being discussed.Style, the way in which it is being discussed. Labeling Standards, the way that the same data is labeled

        Types of shift: Covariate Shift: The input changes but not the labeling. Label Shift: The output dist diff (which also implies the input changes). Concept Shift: The conditional distribution of labels changes (e.g. different labeling standards).

        Generalization: 
            Domain adaptiation-train on many adapt to a target domain at testing. 
                Supervised adaptation: train w/ target-domain labeled data. Unsupervised adaptation: train w/o target-domain labeled data
            Domain robustness-train on many doms perform well on all doms.
                esp minority doms.Zero-shot robustness to domains not in training data.
        Detection: Binary classification: detect whether a test example is an OOD example or not.
        
        Multilingual learning also a multi task learning.

        i.e.: Languages as Domains: Multilingual learning is an extreme variety, different language = different domain
            Adaptation: Improve accuracy on lower-resource languages by transferring knowledge from higher-resource languages 
            Robustness: Use one model for all languages, instead of one for each

        Earlier Method on Multitask Learning Feature Space Regularization: Try to regularize the features spaces learned to be closer to each-other.
            encourage rep learning. classifier to learn class label on top of adversarial feature extractor for downstream tasks. from end of featrue extractior we also put domain classifier to encourage((to check)) domain specific learning(and to avoiding mushing everything/all domain in same space).
            idea: want the encoder to learn representations useful for the task. But also want the encoder not to “overfit” to one domain if you want transfer/invariance
                label classsifier minimizes loss from both feature extractor and label classifier to make feature predictions on downstream labels.
                domain classifier minimizes loss on bomain classifier but maximiszes loss on feature side, to encourage features that are hard for domain classifer to distinguish - domain invariant.
                    i think the idea is if the feature extractor learns representations that confuse the domain classifier, then those features must capture what’s common across domains, making them more general and transferable.
                Encoder basically minimizing label loss while maximizing domain loss so those features hide domain cues and become domain-invariant.
            if domain specifc invariance is good, can be confident label classifier will also be able to generalize better.
            neding underlying similarity to pass down task - cant work in unbalanced, un related, etc. can have neg transfer-degrade perf. need underlying assumption of some similarity/structure to enable transfer. 

        Simpler Multi-task Learning Method(data manipulation): i.e.Adding Domain Tags: add tags to indicate what domain or language let model know aht it would want it to do later. Model still doing task prediciton. introduces a small number of params.

    Instruction Tuning:
        T5: Text-to-Text Transfer Transformer -Supervised training on many NLP tasks, Control a single model to do tasks following the instructions in the prompt.
        T0 model (Sanh et al 2021) from Hugging Face& google: Multitask Prompted Training Enables Zero-shot Task Generalization. i.e. trained on summarization, sentiment, question answering but was able to do inference on natural language inferece
            can improve zero-shot perfomrnace than gpt zeroshot and gpt fewshot.
            FLAN Collection: instruction tuning dataset.
            models ex: Flan T5, LLaMa2 Chat, Mixtral Instruct

    Synthetic Data generations: 
        Self-Instruct: to generate syn data and train model again.
            Models sees all tasks, generates synthetic tasks instructions, then ask model to follow isntruction some classifications, and create outputs(ouptut first/input first), then filtered ans passed back in pool again.
                i.e.Input-first: Ask an LLM to come up with the input fields first based on the instruction, then produce the output. (or can use some label and then ask to generate text based on that label i.e. sentiment, etc)
                    Output-first: Generate the output label, and then generate the input based on the output.
    
        Evol-Instruct:  idea of evolve instruction into more complex instructions.
            filter out invalid instruction, find similar instruction but not smae intent, etc.
                indepth - you can rewrite given prompt into more complex version to make tasks more challengin even though same intent.
                in-breadth evolving - draw inspiration and create a completely new prompt same domain but even more rare.
            used to create new instruction pool to increase reasoning examples, deepening, complicate, concretizie, add constraints, etc.

    Knowledge distillination - large model to small model to run deployable env.

# LECTURE 9: Prompting and Zero-/Few-shot Learning
    Four Paradigms of NLP Technical Development
        ■ Feature Engineering, popular around 2015
            fully supervised learning, neural network based models to train classifier.
            manual features - svm, crf.
        ■ Architecture Engineering - learn feature than hand design feature 2013-2018
            lstms, cnn, gru, etc. dont need to manually define features. modify model architecture.
        ■ Objective Engineering - pretrain, fine tune , 2017-now. self supervised
            bert -> finetuning. less work on architecture but engineer objective functions. self training w contrastive learning,etc.
            still need to init full model and add shallow layer on top.
        ■ Prompt Engineering - pre-train, prompt, predict, 2019-Now.
            engineer prompts to predict in zero shot or a few shot.
            chatgpt, 3,4.

    Prompting: encouraging a pre-trained model to make particular predictions by giving a prompt specifying the tast to be done.
        general workflow: prompt addition, answer prediction, answer-label mapping.

        prompt addition: input and template with x that model predicts, then map predicted x as specific sentiment for example positive.
            cloze prompt  - predict x like before.(w llm)
            prefix prompt - x token placed at end, (use autoregressive movel to predict)

        Design Considerations for Prompting:
            (look at Fig 2: image for considerations)
            ■ Pre-trained Model Choice
                popular choises: 
                    (Left-to-Right) Autoregressive LM (useful for larger scale lms, counting based, good for language generation, works well w prefix prompt, use at test time, no need to train - i.e. GPT 1, 2,3,4),
                    Masked LM: bidirectional, great for NLU tasks. Bert,Roberta, etc. use Cloze prompt, language understanding.,
                    Prefix LM: combination of mased and autoregressive, prefix prompt, Corruption operations can be introduced when encoding X,
                    Encoder-decoder LM: A denoised auto-encode, very similar to prefix lm, 2 transformer model and two different masking. bart, t5.
                        MASS: transformer based - only predict masked spans 
                        BART: Transformer-based - Re-construct (corrupted) original sentences
                        mBART: Transformer-based Multi-lingual Denoising auto-encoder - Re-construct (corrupted) original sentences 
                        UNiLM: Prefix LM, left-to-right LM, Masked LM  - three types of LMs, shared parameters
                        T5: left-to-right LM, Prefixed LM, encoder-decoder - use instruction to explore different downstream objectives respectively(modified data format, to model learns to do multi-task instruction following)
                        Application of Prefix LM/Encoder-Decoders in Prompting:
                                Conditional Text Generation: □ Translation □ Text Summarization,
                                Generation-like Tasks: □ Information Extraction □ Question Answering
            ■ Prompt Template Engineering
                traditionally take input and predict prob in label space. but prompt formulation deinfe input, template and predict z in template. and map predicted token to label(but how to deisgn suitable prompt).
                    prompt shape: cloze prompt, prefix prompt(more popular)
                    prompt template: hand crafted work, automated searches - find optimal objective function, in discrete space, or Continous space(i.e.prompt tunign- learn prompt exameple.)
                        prompt search:
                            prompt mining: early work, find prompts given a set of question/answers: dependency parsing to capture sentence structure(i.e. dependency tree), 
                            Prompt Paraphrasing: paraphrase an exiting prompt to get other candidates - i.e. back translation with beam search (bidirectional translation models, translate and untranslate and try differences to get best output), 
                            Gradient-based Search: i.e. Autoprompt. have tarining example but dont know which prompt will work best. find prompt tokens that maximize performance.
                                Q: (confused.)
                                    basic idea is to automatically find “trigger tokens” (words or short phrases) that, when added to an input sentence, make a pretrained masked language model 
                                    (like BERT or RoBERTa) behave as if it was fine-tuned for a downstream task (like sentiment classification).
                                    Autopromt searching for a few words(tokens) that manipulate model into behaving like a classifier. [input][T][T][T][T][MASK]
                                    after it learns from a few examples, can be implemented into new input examples, to add tokens to make main model predict the masks, the prediction sentiment can be used similar to classifiers.
                            Prompt/Prefix Tuning: obpitmize embeddings of a prompt, add embedddings later as prfix for at testing.

            ■ Answer Engineering
                We have reformulated the task! We also should re-define the “ground truth labels”
                map vocab in answer space to downstream calssification label space. if we can learn maping, we can include mapping in prompt, or post processing(extract & maping leter.) step.
                    Shape: token(single word), span(chunk of texts,usually used in Cloze prompt), sentence(supports structured output like xml, json, used w/ prefix prompt (seq2seq LM for generative tasks)).
                        token,span great for classifcation, sentence great for generation tasks.
                    Human effort: hand crafted(infine space - summarization, machine translation - map predicted tokens to final answer. finite space - text classification, sequence labeling - map finite set of words to labels), automated(discrete - Answer Paraphrasing, Prune-then-Search, Label Decomposition. continous).

                Chain of thruoght prompting - (still linear just chained) Instead of searching for the answer directly, and manually add some intermediate reasoning steps in the prompt to guide the model derive the answer
                    can improve performacne for deduction realteed tasks(i.e. maths). random generation to find path.
                    another version self consistency with COT - multiple COTs and majority vote.
                Tree of thought - Instead of search the answer using a linear chain structure, prompt the output sequence to follow a tree structure
                    still need decoding process to handle performance. if certain branches thought says not possible(validation process, with LM) we prune that branch. allows backtracking
                    can have false negatives - evaluator(LM) might make mistakes. might get stuck in loop(hand design termination), search space too large - slow and challenges api/LM/evaluator calls(too many).
                Graph of Thought: can self look over node, back track, and can aggregate chains to single node. single output(divide-conquer,sorting, etc.).
            ■ Expanding the Paradigm
                Multi-Prompt Learning:
                    Prompt Ensembling:using multiple unanswered prompts for an input at inference time to make predictions . Stabalizing performance, Alleviate the cost of prompt engineering(typical methods:unifrom averaging, weighted averagin, majority voting).
                    Self-Consistency Prompting: ask model to sample diverse reasoning tasks, dif temps & other Hyperparameters during decoding and check final answer to see which answer is dominant.
                    Prompt Augmentation: add examples ot help model understand what objective is.
            ■ Prompt-based Training Strategies:
                Data Perspective: How many training samples are used?
                    zeroshot- without explit training of LM for downstream
                    few shot -  few training samples
                    Full-data -  lots of training samples
                Parameter Perspective: Whether/How are parameters updated?(examples of strategies)
                    (Strategy: parms details - model ex.)
                    Promptless FineTuning: lm params tuned - BERT Fine-tuning
                    Tuning-free Prompting:  GPT 3
                    Fixed-LM Prompt Tuning: Additional Prompt Params + Prompt Params Tuned - Prefix Tuning
                    Fixed-prompt LM Tuning: LM Params Tuned - PET
                    Prompt+LM Fine-tuning: LM Params Tuned + Additional Prompt Params + Prompt Params Tuned - PADA

            General Guideline: 
            If you have lots of training samples?:Promptless Fine-tuning, Fixed-prompt Tuning, Prompt+LM Fine-tuning
            If you have a huge pre-trained language model: Tuning-free Prompting
            If you have few training samples?: Fixed-LM Prompt Tuning, Tuning-free Prompting

        
# LECTURE 10: Learning from Human Feedback 1
    POlicy gradient fundamentals - Reinforce, Policy optimization PPO RLHF standards, etc.

    Great Challenge of LLMS: pretrained llms are great for next token predictors, but not helpful assistants.
        behavioral gaps: factual incorrect, toxic or biased, off topic, not aligned with user intent. Goal birdge the gap.

        Reinforcement learning w/ Human Feedback, train lmms to align model outputs with humans preferences/intent/values(environment). (OpenAI did this)
            pivotal component  in success of modern instruction tuned llms, instructGPT< ChatGPT 4, Claude, Llama 2/3, deepseek math.
        
        pariwise preference feedback: ask user to choose between two responces, i.e. Chatgpt.
            RLHF applications - Helpful& Harmless, Toxicity and bias reduction, ethical alignment, Political neutrality. 
                - personalization, persona, roleplaying, Cultural & Linguistic alignment(politeness, user style respones i.e. laymans vs professional), domain specific adaptation(align w/ domain experts law/medical/etc.)
                - resoning - prefer more failthful resoning chain, planning optimize long term task success AUtoGPT style agents with RLHF loops.   
                LLMS as chatbot - without environment reward are stiill hard to get meaningful responses during Human comp interaction. Challenging to simulate an evironment in an online setting.
                    - InstructGPT-earlier version of GPT, superfized finetunign + reward dataset, then RLHF to make model.
                        dataset - plain: ask labellers to come up with arbitary task with diversity, few-shot: ask labelers to come up with Q|R pairs of instructions. User absed: a number of use cases they asked labellers to come up with querys for these data sets.
                            - 3 datasets: SFT data set ask labelers to write resp to given prompt. RM data: labelers give ranking of model output used to train the reward model. PPO data: data collected during RLHF without human labels, using RM to provide rewards. all colelcted for replay purpose.
                            SFT - pretrained gpt3 model on finetune model to instructure and finetune. cross entropy to forcemodel to follow trajectory.
                            RM:  collect comparison data and train a RM. smaller lM to rank K respones to  a prompt, pairwise comparison, pariwise ranking loss used.
                            3rd step RL: train RL policy model initialized from SFT policy model(continue from checkpoint end of SFT), using PPO algo(stable RL model), maximize rewards and Kl penalty from SFT model.(checkpoint from RL close to SFT checkpoitns to not forget instruction following)+LM pretraining on public NLP datasets
                            pretty much 3 steps used to train earlier chatgpt models.

        Reinforcement learning:
        REINFORCE - montecarlo version of policy gradient.
            Markov Dcecision process(during training) - LLMas agent(looks at state), action is reponse based on policy, environment is human, and feedback is given by state(query+resp) and rewards, 
                - state updaed as conversaion goes on. process repeats forming a trajectory(episode) of horizon t(generated long seq of dialogue history). reward uses sum of trajectory * discout factor. has artificial ending state, predefined - no rewards used ended (after deployment can sitll be adjusted to run forever).
                MDP has 5 tuple - state space(all dialogue), action space(all model can generate), environment transition(prob of moving to new state after perforing action in state),
                    Reward func(immediate expected reward after taking action in s), Discount factor(how much future rewards are valued with respect to immediate rewards)
            policy is LM and environment transition(user-unpredictable/unmodelable) is independent of policy - after llm responds or user follows up.
                goal is to maximize the expected return under policy to update gradeint by gradient ascent. hard to first compute rewards then compute gradient - expensive.
                    -log derivative trick - has additive property, compute gradietn in tackable way.
                    policy gradient - include gradient of polity at time t and weight it bu total reward of full trajectory tau.
                    gradient written with - Reward to go: using full reward for each time step causes high variance. by caulalty action cannot affect past rewards, so we replace full reward by total return from time t till end of episode. 
                        using exponentially Discounted reward to put more emphasis on recent times, and less towards future(which are less concreque), math wise discount reward would be bounded for stability, and to define markov recursion or is by definition.
                        basic idea of REINFORCE.
                    adding base line to reduce variable without bias. cumulative reward used as baseline to compute advantage(reward-baseline) to ground responses to a baseline(for easy vs hard tasks).
                        weight our likelihood by Advantage. adding baseline does not affect bias for expectation. does not introduce bias but cna reduce variance.
                        Designing baseline - predict final reward to predict current state(need to learn baseline function given params): can be done sentence level one baseline per sentence, or Decoder state level - one baseline per output action. Option 2: mena of rewards in minibatch - used in 1990, but GRPO Recurrent too(given prompt sample several response, and use mean of all rewards in mini batch).
                            Actor-Critic - another common choice using state value function yielding the advanctage, - critic model is trained to estimate state value function PPO uses AC formulation.
                        Want to sample multiple episode under current policy and compte avg gradient for more stable accumulative reward.then update policy by SGD, since we are working in mini-batches.

                    Practical tewaks - REINFORCE is simple and unbiased but often high variance and sample inefficient, 
                        increase batch size, norm, entropy and regularization are important hyeper params, LR and baseline quality too.
                        using bootstrapping(Actor-critic), baselines, reward-to-go, advantage estimators, and variance reduction techniques greatly improves performance
                    
# LECTURE 11: Project Pitch Presentations

# LECTURE 12: Learning from Human Feedback - aligning llms with human intent and valies. (continued)

    Proximal Policy Optimization(PPO)
        PPO uses actor critic -  a more stable variant of policy gradient
        idea is to replace accumilative reward with advantage. expected advantage weighted by policy. instead of fixed function we use a value paramerized func.
        A critic (value function) estimates the value function that quantifies the expected reward (scalar) for a given state
        A actor (policy) estimates the probability of an action given a state
        Why? - REINFORCE is high-variance and unstable even with actor-critic, when we take large gradient step it can cause perfm forgetting.
        a trust region: “Don’t move the policy too far in one update.” This leads to a thread of methods called Trust Region Policy Optimization (TRPO)
            PPO is a simpler, practical version of TRPO that constrains updates to avoid drastic policy changes.
            replaces he hard KL constraint with clipped surrogate objective. when optimizing expected advantage weihgted by ratio, we would containt ratio within a range.
                if smaller or larger, then model is moving too far away, we clip it to a bound. with epsilon Hyperparameter, typically 0-1.
        Full PPO obj - Policy loss(clipped surrogate) - value function loss(sq error) + entropy bonus for exploration.
        cliping ensures stability and avoid large steps, simple no second order approx, rertains sample efficiency on policy methods, 
            the advantage baseline keeps the gradient unbiased while reducing variance

    Group relative policy optimization (GRPO)
        PPO having a crit doubles memory/compute and value estimation may introduce variance or error.
        GRPO removes need for separate critic by estimating a baseline using group samipiiling - generate multiple outpits per prompt
            and compare relative to each other(uses mean and var of multiple rewards to normalize multiple advantage)
            [comparison image shared Fig3.]
        GRPO uses group normalization of rewards to define advantages across the outputs in one group: ONE IS GIVE SINGLE REWARD PER OUTPUT(OUTCOME SUPERVISION), or GIVE REWARD FOR EACH PROCESS STEP OF AN OUTPUT(PROCESS SUPERVISION)
            OUTCOME SUPERVISION: compute group mean and std, each output define norm reward, that is used a advantage for every token in ouput. (method just an approximation, does not need to be accurate)
            PROCESS SUPERVISION: rewards at each reaosning step. (can be 1 token or several grouped as reasonign step), rest same as before but applied to steps. can give reward middle of environment/sequence instead of only at end.

        By normalizing within each group, GRPO encourages relative improvement rather than absolute. Even if all outputs are “bad,” it still picks the least bad ones to promote.
            It reduces variance and avoids needing a value critic. 
            challenge (discussed in follow-up works) is when all sampled outputs are equally bad (e.g. all wrong) — then the group normalization gives zero advantage and no gradient signal (an “all-negative group” problem) 
            Summary:
                removed critic - no need to estimate value function.(benefit, robust across group)
                still needs reward function even though no reward model.
                uses mean rewards to estimate advantage.
    
    Direct Preference Optimization (DPO)
        removes the need to train a separate reward model and then run RL. Instead, it directly learns from human preference comparisons (pairs of chosen vs rejected) in a supervised-style optimization.
        offline Rl algo, used fixed LM model to generate output use humans to collect pariwrise pref data. This dataset is used to do DPO training.
            (online for example, LM model is always updated with rewards from pretrained reward model.)
            ask human to collect a lot of data, and use that to update policy without needing a reward model.
            Key takeaway: no explicit reward model, no RL loop, no value critic — it’s just a pairwise preference-based update.
            much more computationally efficient.

            Pros:
                Offline - simpler  (no reward model training, no RL iterations, training policy model learning impricitly)
                More stable (fewer moving parts).
                Empirically competitive with PPO-based RLHF in many tasks 
            Cons:
                Assumes pairwise preference data (cannot trivially incorporate more general feedback)
                Might be less flexible in incorporating more complex reward structures
                
    [High level comparison for RLHF shared FIG4.]

    Research on field here:
        Preference learning rewards modeling for value alignment, and reward for reasoning models as well.
        research going on to combine direct pref methods with Rl based.
        scalability and efficiency
        multi object alighnemtn, safety 
        open benchmarks, eval.
        theory & convergence anlysiis

        Pluralistic Preference Learning: human pref diverse and distributional, but llms are skewed to one TRUE direction.
            Train reward with conflicting reward, where preference is switching, DPO skews towards majority pref instead.
            Proposed distributional pref optimization GDPO, use distribution to calibrate model to not be monotonic. 
                uses belief ratio of pref used to weight in on proposed, collected when collecting pref data as well.















Course: https://junjiehu.github.io/cs769-fall25/lectures/ 