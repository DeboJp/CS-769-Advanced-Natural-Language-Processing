# LECTURE 2
    Lexical Semantics: Meaning of word
        lemma(title), sense(different meaning within diff context), definition
        connections between words (synonyms, opposites, etc)
        taxonomy - absract(general words) to concrete(specific).
        Semantic frames and roles -> to denote perspective or participants in an event.
        
    Problems with discrete repr:
        one hot vector(give each word separete numbers on vectors for 1): sparce, subj, expensive, hard to guess relationships.
        goal: to map similar meaning workds closely, can use KNN to find close words. Standard way to repr mean ing in NLP.

    Training of word embeddings:
        Wrod2vec: 
                Cbow(opp of skip gram) - from many guess one.
                Skip Gram: use imput to geuss bunch of words (define a sliding window and train a classifier with padding, 
                            to predict correct token for all sectences. roughly self supervised.)
                            - some words used a lot, learn a lot of repr in language.
                            - others used less, more concrete, meaning very unique.
                            uses ONE HOT + embeddings to train/predict surrounding/similar word repr.
                            - loss function log probablity(dot product to predict similarity between surrounding words and 
                                    normalize with all words -> expensive so approx by negative samplinig(sampling without main 
                                    sliding window but vocab is not all unqiue - but can use freq for norm) idea grab random samples 
                                    to do sum of pairwise normalizaiton -> convert to binary classification-easier to optimize and 
                                    learn repr) then use some gradient decent to predict.
                            - evaluation: check quality: instrinsic eval- test repr align with intuition about word meaning, using cosine similarity of word embeddings.
                                        extrinsic eval: test weather repr good for downstream tasks, such as taggin, parsing, QA or some task used by some model.
    Text Classificaiton: generative vs discriminative.
            generative - prob joined with and ouput, argmax over joint prob.
                - prob of input first, probably more expensive.
            discriminative - prob output given the inout data, argmax over conditional prob.
                = directly models prob with output space.
                - BOW(bag of words) discriminative model, does not care about word order. get score to prob then gradient descent. but for neural networks can use sequential order.
                
            logistic regression good for binary classification. sigmoid for binary, softmax for muti-class to repr to valid probabilty.
            gradient descent - gradient of loss function and use chain rule. to move to direction that decreases the loss. lr is the speed of movement.

    Q: norm samlnig denom is some sort of avg to shorten error?
    Q: so do we use freq when norming or no? since Vocab is not unique?
    Q: how does input become smaller dim(PCA?)?

    HW1 classification example:
        Continous bag of words(CBOW) has lower dimension, just bag of words(BOW) is one hot size of each lookup is number 
        of vocab. + linear layers. then softmax function and optimize with gradient descent and regulizer.
# LECTURE 3 : Language Modeling
    Ngram Language model, Neural Model, Evaluation

    define: final end of sentence token is special. 
        problem is probability of grabbing sentence from 

    What can it do: score sentences(high/low), or generate sentences(prob: of next work/token)

    N-gram Language model:
        every word is independence, and can use maximum likelihood MLE to see how likely eaach word is supposed to appear(by counting).
        - allows prediction of whole sentence too.
        - want to score with unknown read words,  interpolations w/ UNK model enables to allow low frequency unknown words.

    higher order model: limit context length to smaller window(n-1). MLE is count and divide. linear interpolation to deal with zero counts.
        sometimes add one smoothing, every word adds 1 count.

        other smoothing techniques: additive, discounting, kneser-ney. most of them to address zero count issue.
    
    Limitations of Ngram:
        Ngram cannot share strength among similar words, need class based language models.
        cannot condition on context with intervening words. need skipgram language model.
        cannot handle long-distance dependencies, need cache,trigger, topic, syntatic moels, etc.

    Linear models:
        cant learn feature combinations, logical errors - cant be expressed by linear features, use neural language models.

    Neural ngram model(since proximating current based on previous, but using neural model to learn embeddings):
        convert word predition to discriminative text classification.

        feed-forwrad neural language models. - uses previous words, looks up word embeddings, combine linear vector to get intermediate 
        vector + bias to get score and then get probability to get next token.
            (grab two d dim vectors and grad a d vector absed on that, convert to v(vocab) dim, softmax to prob)
        
    
        Solves: simililar word/context issues, intervening words, cannot handle long-distance dependencies.

    neural networks allow design of complex functions.

    PROS ngram: models are extremely fast, better at modeling low frequency - TOOL: kenlm

    ngram lm(non parametric-non training just statistics), prefix context table and counts will be massive to store. to compute just retrieve row.

    neural counts but also compresses, memorizes and generizes.

    Non-parametric: • Memorize low-frequency pairs of (prefix, next word) • Cannot generalize to unseen pairs • Interpretable & easy for incremental updates (insert/delete/replace)
    Parametric: • Learning from data-driventraining • Good generalization tounseen pairs • Black-box & expensive forparameter updates withforgetting issues

    Why llms so powerful: 
        - Architecutre, MLP then RNN and now transformers to capture sectquecnes and effecienceis to scale to 100B+.
        - Pre-training on massive raw texts.
        - Fine-tuning on language intrustiosn with supervised learning or RLWF.

    LM Evaluation:
        - log prob of sentence over test corpus, aggregate them log likelihood score(not comparable to different test sets), so we normalize it by count, turn into per word cross entropy and grap perplexity.

        unknown words: 90% of words are low freq, cut size of corpus(by unk or rank threshold), treat 90% as unknown words, to save compute.
            - subword tokens can handle loss of accuracy by using sub tokens instead of throwing tokens away, also handles mispellings.
        
        IMP: cannot compare models over different vocab. 
            Or more accurately, all models must be able to generate the test set (it’s OK if they can generate more than the test set, but not less)
            • e.g. Comparing a character-based model to a word-based model is fair, but not vice-versa

# LECTURE 4: Recurrent Neural Network 

    General knowledge:
    - when working with minibatches. need to add artificial token to make sure each sentence has same length. add paddings. (PER BATCH ONLY)
        - when computing avg we should zero out padding tokens, sum out embeddings over word and divide by number of real tokens-non paded length.

    Sequential:
        long distance dependencies in language can be compicated, nouns, or references such as 'it'.
    
    Recurrent Neural Networks:
        Approaches:
        1. variable size inputs: add zero paddings and concatate + Linear layer -> simple but downt works for longer sequences. massive ff layer.
        
        2. feed one word to each feed forwawrd layer : for each token dot product with weight + bais, and pass to next token. 
                (idea compress and pass info to next token to carry on, avoids super long W matrix.)
                do not need to pad senteces, and each ff layer is smaller than previous method.
                maiin idea for RNN, we can have many layers as we want, variable depth network. remembers information along time dimention.
                we can predict per token vs label, grab loss and sum them up for total used in backprop. backprop updated weights(rnn) not inital hidden layer passed on.
                backprop over time sequence - issue vanishing/exloding gradients. if we do back prop since al weights are shared.
                    - handling vanishing gradient: LSTM - long short term memory. make additive connections between time steps. gate to contril info flow, and 2 mem cell to store short/long term info.
                        - forget gate-how much forget long time mem, input gate-how much to add to long term mem, compute new values to add to memory, output gate-how much to pass on from long term to short(next step).
                        - all is passed to next time step(short term - only some portion of long term-higher freq of updating, allows prediction per token in seq).
                    - handling exploding gradient: gradient clipping(general idea can be applied to others). we norm by some number before updating, basically making it take shorter steps, instead of massive ones in gradient descent.

                exposure bias/distributional shift: training we pass in ground truth per token, but during test we pass on predicted words to predent/finish sentence. since its a prob dist model might sample an unknown sequence.
                    - happens for generative tasks, not classification.
                    - to handle:some reinforcement learning or scheduled sampling. first train on ground truth for some iter but gradually switch to taking its own input, and always continue like that. 
                                        - cannot start with pass in its own input, or else it will learn garbage. start with supervised then reinforcement function.
                                        - handles model robustness to handle new sequences, not necessarily accuracy. 

                                        Q: do we have to write our own feedback func to say weather new seqs in training during exploration are good or not?
                                            - model is baked in to explore some similar but wrong words but should later still learn the proper sequence. so overall message is not lost. not necessarily RL but SL + noise.
                
                Applications:  represent a whole sentence to make pred, represent a context within a sentence(read context up intil that point).
                                - can do 1-1, 1-many, many-1, many-many, many to many fully conencted.
                                    image captioning, sentence classification, machine translation, sentance tagging.
                                
                                - can do binary or multi class prediction.sentence repr for retrival, sentence comparison, etc.
                                - sequence labeling, calculating reprs for parsing.
                                - language modeling - tagging task, each tag is next word. is a auto regressive model(curr depends on prev token.)

                                bidirectional rnns - to handle two directional languages, also llms use it, also useful for sentence repr.


                What can LSTMS learn: 
                ...

                Efficiency tricks: minibatching makes thigns faster. harder in rnns than feed forward networks. since each word depends on prev word. (better to separe padding and end of sentence token)
                                - using a masking token to zero out paddings. to exclude it when calc loss. and divide sum of loss by real tokens in minibatch.
                            
                            bucketing/sorting - put sentences of same length with same minibatch. but can result in decreased performance.
                
                handling longer sequences: cant fit massive lang in mem, truncated segments. separete it into 2 passes but connect end of 1st to start of 2nd's hidden layer.
                                            - backprop individually though.
                
                Gated Recurrent Units: LSTMS but way more efficient. with better parameters.
                architecture search - basic lstms pretty good(the various combinatoins gates searched.)

                Multi-Scale Pyramid RNNS: one for word, one on top for sentences, one for paragraphs.

# LECTURE 5: Attention and transformers
    Encoder - Decoder models: used to be popular, nowadays only decoder(encoder merged within). prob, compressing sentences of varying length to single vector, hard to decode later on.
        - idea use mulitple vectors bsed on length of sentence: attention.
    Attention: embed vocab to dict, query a target element, pick relevant sources by comparing query/key-value based on confidenc. summarize relevant work tokens into a contetxt vector, used to predict target token.
        encoder gives keys and vals to each word based on learned functions, decoder creates queries to check/find relevant key. we dont use argmax on keys (not differentiable(is discrete)) instead use softmax (to smooth attention dist), is differentiable, allow backprop.
        after grabbing query results, we grab norm attention weights, create a context vector with weights + base word vectors, and pass it though a FF network + predicting base word vectors to predict per label. 
            Feed Forward:
            - multi layer perceptron great for large data (applied position-wise after attention)

            Score (attention alignment):
            - dot product; if too large dim, use scaled dot product
            - bilinear (general form): score(q,k) = qᵀ W k
            - MLP/additive (Bahdanau): score(q,k) = vᵀ tanh(W_q q + W_k k)
    
    transformers: adv: really fast, only matix, parallel sequetial processning, efficeint.
        - self-attention - allwos parallel comp of all tokens.
            attends to itself most, but also givs attention to other words/elements in sentence, context sensitive.
            using embeddings to grab key,query,value. use querry to grab ino from each other embedding, use softmax on all. use values and softmax dist to grab sum. saically summarizes its own info and other words info. can stack multiple self-attention layer.
        - multi headed attentions - allows querying multiple positions at each layer.
            multiple attention heads focus on different parts of sentence(can encode in parallel).used several heads per set, t heads to create a single d(t) dim vector.
            add nonlinearity - FF(MLP), so output is no longer liner to input, add capacity to model to make it complex. we stack self-attention(multi head) and non-linear layer N times to increase capacity.
        - position encoding - addds position ino for each token.
            model perspective no position info, since parallel. add soe info function at the very beginning to augment input where word is location.
            naive = integer index and another dim to input. issues embedding vector also numeric, so may cause instability.
            we use frequency based functions, sigmoids with diff frequency. differnet dimentions say differnet info. 1st even odd indicator, layer first-half etc. each word have added dims. combination if some of those dims give positional info. typically concatate with word embedding info and pass them as same.
            another idea add learnable parameter for every input. but fixed size for length, need to estimate or model might not work. typically more optimal but more complex.
            typicall absolute not useful, relative to other words more useful.
        - adding non-linearrities - combiens features from self-attention layers.(expl above)
        - masked decoding - prevents attention lookups in the future tokens, to avoid predicting future tokens.
            avoids looking into future tokens, masks attention to future tokens(keys).(step before making softmax dist). future token will not affect context.

        self attnetion: combine words
        multi-head = learn independ head information
        norm dot prod atten - to remove baias in dot prod calc
        prod encoding - without RNN, to udnerstand location.

        Training tricks: layer norm - avoid number blowouts, training schedule - adjust from adam (study show needs some warmup from small to large then exp decay to find optimal lr), llabel smoothing - insert some uncertainty to training proc(allows sampling since finding a single point is rly hard).

        Attention hard - slow at test, doesnt necessarily outputform RNNs on decoder side for seq2seq taks (transformer need a lot more data), hard to train on small data.

        EXTRAS:
        Additions: can add markov properties i.e. add attention matrix. encourage sparcity: instead of softmax to smooth out, ask model to pick a few tokens instead each time.
            incorporating markov properties: attention from last time correlated with attention from this time. add attention from last for next deision.

            Hard attention: instead of softmax, use zero-one decision. harder to train, needs Reinforcement learning, perhaps helps interpretability.

        Better Training: 
            neural models tend to drop or repeat content, add penanty if attention not 1 over each word.

            attention is not alighment, often blurred, off by one on matrix for translation, and other tasks too.

            supervised - pretrained + supervised is strong alignment model. but tons of data, model can generally learn too.

        What else:
            Copy Mechanism: model can not only generate from vocab, but also copy tokens directly from input/history, final prob = mix of generate prob + copy prob

            hierarchical attentions: attention over words then attention over sentences then document. grab context of document.

            attention allows various modalities, vision, text, sound/freq, etc.

            multi source attention.

            basic idea of attention: idea similar to info retrival.

# LECTURE 6: Pre-traind Sentence and Contextualized Word Reprs.

    why pre-training, 

    non-Contextualized - word2vev, seq ,etc.
    contextualized - bert etc.

    pass non-context though an multi layer perceptron to turn into contexualized?

    why pre-trainng: learn multi-tatsk learning, transfer learning - finetuning

    NLP TASKS: text - language modeling, Natural occuring data - machine translation, hand-labeled data - most analysis tasks.
        - 1. Perform multi-tasking when one of your two tasks has greatly fewer data, 2. Perform multi-tasking when your tasks are related

    multi-task leaning, train reprs to do well of multiple tasks at once, i.e. tagging, language modeling.
        - should be table to train encoder on large dataset for one obj, then use same encoder+finetuning later for other tasks such as tagging.

    3 main objtives - define model, define training obj, data.

    end-to-end - is done if easy or has enough data points. else always use pre-training.

    Usage of sentence reprs: 
        Sentence Classifcation - sentiiment, classify, binary labels over sentence, etc.
        Paraphrase identification/Semantic Similarity - compare meaning semantically between phrases
        Textual Entailment - learn implications(Entailment), Contraditions, Neutral.

    Pretraining Techniques:
        (idea model pretrained for cetain task, then encoder/embeddings used from that to be used in another task)
        eg 1: Google: model: LSTM, obj: LM obj, data: Classification data itself or amazon reviews. downsteam on text classficaion, init weights and continue training.
        eg2: GPT: Masked self-attention(transformer), LM obj, Data: BooksCorpus. downstream some task finetuning other tasks additioanl multi-sentence training.
        eg3: Auto encoder + Transfer: LSTM, from single sentenve vec re-construct the sentence. Data: clasification data itself or amazon reviews. downstream on text classification init weights continue training.
        eg4: Skip-thought vectors(sentence level contet): lstm, predict surrounding sentences, data books, imp because of context. downstream sentence pair classification. 
                (Similar to Skip-gram that predict the surrounding words by the center words)
        eg5: paraphrase based contrastive learning: try several model, predict weather two phrase are paraphrases or not, data: paraphrase db from bilingual data.downstream: sentence similariy, classification, 
                (LSTMs work well on indomain but word averaging generalizes better.)

        deep-fusion - concat sentence for grabing self-attention(sentiment - close, deep,repr, efficient)
        late-fution - do encoding on ecah then gram.(sentence classification - sparse, more time?)

        eg6: Entailment + Transer:Bi-LSTM + max pooling, (use human labels for pretraining) supervised training for a task such as entailment learn generalizable embeddings. Data: Stanford NLI, MultiNLI, 
                (Tends to be better than unsupervised objectives such as SkipThought)

    Context word repr:
        Central Word Prediction:
            context2vec: Bi-directional LSTM , Predict the word given context, data: 2B word ukWaC corpus. downsteam  use vectors for sentence completion, word sense disambiguation, etc
            ELMo: Model: Multi-layer bi-directional LSTM • Objective: Predict the next word left- >right, next word right->left independently 
            Data: 1B word benchmark LM dataset. Downstream: Finetune the weights of the linear combination of layers on the downstream task

    Masked Word Prediction:
        Masked Word Prediction (BERT):model: Multi-layer self-attention. Input sentence or pair, w/ [CLS] token, subword representation, obj: Masked word prediction(mask certain words ask predict) + nextsentence prediction((are they close or far))
            Data: BooksCorpus + English Wikipedia (16GB)
        
            select 15% of words randoly, replace with mask 80%. time, 10% replace random(to simpulate noise), 10 no change. predict all maskd words given surrounding context. like context2vec but better suited for multi-layer self attention.

        Consecutivee Sentence Prediction: classify two sentence as concsec or not. data = 50% of trainngin from openbooks is consecutive, use unique cls token to include meaingn of two sentence. mask some sentece in sentence and sep tokens to separaee sentences.  and binary predictions.

        Hyperparameter Optimization/Data(RoBERTa) - same as BERT, but trainlonger and drop sentence prediction objective. but with much larger data. empirically much better.
            - sentence prediction is easy(doesnt help) if mask token prediction is done well(hard-loss goes down slow).

        Distribution Discrimination -ELECTRA: same as bert,Sample words from language model, try to discriminate which words are sampled. discriminator used as word encoding model later on.
            (masked sentence passed in generator, which predicts/sample words, discriminator then predicts if original word passed or not.) 
            (better than bert, more gradient updates to model since per token labeled, bert only does 50% as their tasks masks 50% only)

        Permutation-based Auto-regressive Model+ Long Context(XL-Net):Same as BERT, but include longer context, Predict words in order(itself and before), but different order every time(train sentences with different permutation orders: some noise but able to learn differnt perspectives). Data: 39B tokens from Books, Wikipedia and Web 
            - later version, instead of shuffing randomly, it group phrases to avoid random places, avoid learning wrong word combinations in sentence.
        
    Compact Pre-trained Models: 
        ALBERT (Lan et al. 2019): Smaller embeddings,and parameter sharing across all layers, but the same inference time as the BERT counterpart 
        DistilBERT (Sanh et al. 2019): Train a model to match the distribution of regular BERT
   
    SUMMARIES:
        Which model:
            - Averaging word embeddings can outperform more complex models, especially on out-of-domain data.
            - BERT shows the strength of bidirectional transformers but does not benchmark against LSTM models like ELMo due to lSTMs being constrained by computational efficiency and scalability.
            - With BERT-like training data, their ablations show architecture and training tweaks can yield further improvements.

        Which Training Objective? chatgpt
            - When using the same data, bidirectional language modeling outperforms a machine translation encoder.
            - Next-sentence prediction, combined with masked LM, seemed helpful in the original BERT paper(boosting perf), but later work (e.g., RoBERTa) found it unnecessary and even limiting when scaling training.
            - Next-sentence prediction provides little to no benefit and can be dropped.

        Which Data?
            - Preliminary results suggest more data generally improves performance.
            - Scaling with large web data yields gains but inconsistently.
                Data with context is probably essential/best.

        Pre-trained Large Language Models: 
            GPT-3: similar to GPT-2, but pre-trained on lot more data using autoregressive LM objective. Demonstrate good few-shot in-context prediction ability.
            ChatGPT: init form GPT-3 fine-tuned with Reinforcement Learning from Human Feedback (RLHF).